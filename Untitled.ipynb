{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 5, 5, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "reshape_flat (Reshape)           (None, 25)            0           main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "repeate (RepeatVector)           (None, 9, 25)         0           reshape_flat[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "permute (Permute)                (None, 25, 9)         0           repeate[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "reshape_back (Reshape)           (None, 5, 5, 9)       0           permute[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2D_0 (Conv2D)                (None, 5, 5, 16)      1312        reshape_back[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "normalize_0 (BatchNormalization) (None, 5, 5, 16)      64          conv2D_0[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2D_1 (Conv2D)                (None, 5, 5, 32)      4640        normalize_0[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "normalize_1 (BatchNormalization) (None, 5, 5, 32)      128         conv2D_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "deconv2D_0 (Conv2DTranspose)     (None, 5, 5, 16)      4624        normalize_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "normalize_2 (BatchNormalization) (None, 5, 5, 16)      64          deconv2D_0[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "deconv2D_1 (Conv2DTranspose)     (None, 5, 5, 9)       1305        normalize_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "normalize_3 (BatchNormalization) (None, 5, 5, 9)       36          deconv2D_1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "add (Add)                        (None, 5, 5, 9)       0           reshape_back[0][0]               \n",
      "                                                                   normalize_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "activation (Activation)          (None, 5, 5, 9)       0           add[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "subpixel_conv (Subpixel)         (None, 15, 15, 1)     738         activation[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 12,911\n",
      "Trainable params: 12,765\n",
      "Non-trainable params: 146\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Conv2D, Conv2DTranspose, BatchNormalization, Input, Flatten, Dense\n",
    "from keras.layers import Reshape, RepeatVector, Permute, concatenate, add, Activation, Dropout, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras_utils.subpixel import *\n",
    "import numpy as np\n",
    "\n",
    "inputs = Input((5, 5, 1), name=\"main_input\")\n",
    "\n",
    "base = Reshape((5*5,), name=\"reshape_flat\")(inputs)\n",
    "base = RepeatVector(9, name=\"repeate\")(base)\n",
    "base = Permute((2, 1), name=\"permute\")(base)\n",
    "base = Reshape((5, 5, 9), name=\"reshape_back\")(base)\n",
    "\n",
    "x = Conv2D(filters=16, kernel_size=3, \n",
    "           strides=1, activation='tanh', padding='same', name=\"conv2D_0\")(base)\n",
    "x = BatchNormalization(name=\"normalize_0\")(x)\n",
    "x = Conv2D(filters=32, kernel_size=3,\n",
    "           strides=1, activation='tanh', padding='same', name=\"conv2D_1\")(x)    \n",
    "x = BatchNormalization(name=\"normalize_1\")(x)\n",
    "x = Conv2DTranspose(filters=16, kernel_size=3, \n",
    "                    strides=1, activation='tanh', padding='same', name=\"deconv2D_0\")(x)\n",
    "x = BatchNormalization(name=\"normalize_2\")(x)\n",
    "x = Conv2DTranspose(filters=9, kernel_size=3, \n",
    "                    strides=1, activation='tanh', padding='same', name=\"deconv2D_1\")(x)\n",
    "x = BatchNormalization(name=\"normalize_3\")(x)\n",
    "\n",
    "x = add([base,x], name=\"add\")\n",
    "x = Activation('tanh', name=\"activation\")(x)\n",
    "predictions = Subpixel(filters=1, kernel_size=3, r=3,activation='tanh', padding='same', name=\"subpixel_conv\")(x)\n",
    "\n",
    "generator = Model(inputs=inputs, outputs=predictions)\n",
    "generator.summary()\n",
    "generator.compile( loss='mse', optimizer=keras.optimizers.adam(lr=0.001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inputs = Input((15, 15, 1), name=\"main_input\")\\n\\nx = Conv2D(filters=16, kernel_size=3, strides=2, padding=\\'same\\', name=\"conv2D_0\", activation=\\'relu\\')(inputs)\\nx = Conv2D(filters=32, kernel_size=3, strides=2, padding=\\'same\\', name=\"conv2D_1\", activation=\\'relu\\')(inputs)\\n\\nx = Flatten(x)\\nx = Dense(x, activation=\"relu\")\\n\\nm = Model(inputs=inputs, outputs=x)\\nm.summary()\\n\\n\\ndiscriminator.compile(loss=\\'binary_crossentropy\\', optimizer=keras.optimizers.adam(lr=nn_param.learning_rate), metrics=[\\'accuracy\\'])\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''inputs = Input((15, 15, 1), name=\"main_input\")\n",
    "\n",
    "x = Conv2D(filters=16, kernel_size=3, strides=2, padding='same', name=\"conv2D_0\", activation='relu')(inputs)\n",
    "x = Conv2D(filters=32, kernel_size=3, strides=2, padding='same', name=\"conv2D_1\", activation='relu')(inputs)\n",
    "\n",
    "x = Flatten(x)\n",
    "x = Dense(x, activation=\"relu\")\n",
    "\n",
    "m = Model(inputs=inputs, outputs=x)\n",
    "m.summary()\n",
    "\n",
    "\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.adam(lr=nn_param.learning_rate), metrics=['accuracy'])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 8, 8, 32)          320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 5, 5, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 3, 3, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 3, 3, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 2305      \n",
      "=================================================================\n",
      "Total params: 390,913\n",
      "Trainable params: 390,529\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "img_shape = (15, 15, 1)\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(BatchNormalization(momentum=0.8))\n",
    "model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(BatchNormalization(momentum=0.8))\n",
    "model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "img = Input(shape=img_shape)\n",
    "validity = model(img)\n",
    "\n",
    "model.summary()\n",
    "discriminator = Model(img, validity)\n",
    "\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.adam(lr=0.001), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = Input(shape=(5,5,1))\n",
    "img = generator(z)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The valid takes generated images as input and determines validity\n",
    "valid = discriminator(img)\n",
    "\n",
    "# The combined model  (stacked generator and discriminator) takes\n",
    "# noise as input => generates images => determines validity \n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=keras.optimizers.adam(lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10131, 5, 5, 1)\n",
      "(10131, 15, 15, 1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"2D_SPH/scenes/tools\")\n",
    "\n",
    "from dataset import Dataset\n",
    "\n",
    "src_patches_path = \"2D_data/patches/lowres/sph_2D_v02-01_d%03d_%03d\"\n",
    "ref_patches_path = \"2D_data/patches/highres/ref_sph_2D_v02-01_d%03d_%03d\"\n",
    "\n",
    "train_data = Dataset(src_patches_path, 0, 18, 5, 15, ['sdf'], ref_patches_path, ['sdf'])\n",
    "test_data = Dataset(src_patches_path, 18, 20, 5, 15, ['sdf'], ref_patches_path, ['sdf'])\n",
    "\n",
    "print(train_data.data.shape)\n",
    "print(train_data.ref_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.001038, acc.: 100.00%] [G loss: 1.380752]\n",
      "1 [D loss: 0.001851, acc.: 100.00%] [G loss: 1.133776]\n",
      "2 [D loss: 0.002171, acc.: 100.00%] [G loss: 0.420289]\n",
      "3 [D loss: 0.473408, acc.: 71.88%] [G loss: 11.655730]\n",
      "4 [D loss: 0.000553, acc.: 100.00%] [G loss: 16.118095]\n",
      "5 [D loss: 0.962623, acc.: 71.88%] [G loss: 16.118095]\n",
      "6 [D loss: 0.241745, acc.: 81.25%] [G loss: 16.118095]\n",
      "7 [D loss: 0.001368, acc.: 100.00%] [G loss: 15.680832]\n",
      "8 [D loss: 0.000858, acc.: 100.00%] [G loss: 15.594702]\n",
      "9 [D loss: 0.001233, acc.: 100.00%] [G loss: 14.450577]\n",
      "10 [D loss: 0.296111, acc.: 96.88%] [G loss: 13.671122]\n",
      "11 [D loss: 0.000026, acc.: 100.00%] [G loss: 13.271968]\n",
      "12 [D loss: 0.000012, acc.: 100.00%] [G loss: 13.226284]\n",
      "13 [D loss: 0.000221, acc.: 100.00%] [G loss: 12.462976]\n",
      "14 [D loss: 0.001075, acc.: 100.00%] [G loss: 12.577853]\n",
      "15 [D loss: 0.000027, acc.: 100.00%] [G loss: 12.064657]\n",
      "16 [D loss: 0.000016, acc.: 100.00%] [G loss: 12.451273]\n",
      "17 [D loss: 0.000079, acc.: 100.00%] [G loss: 11.720919]\n",
      "18 [D loss: 0.000009, acc.: 100.00%] [G loss: 9.867696]\n",
      "19 [D loss: 0.000066, acc.: 100.00%] [G loss: 11.643980]\n",
      "20 [D loss: 0.000849, acc.: 100.00%] [G loss: 12.500009]\n",
      "21 [D loss: 0.124211, acc.: 96.88%] [G loss: 11.789725]\n",
      "22 [D loss: 0.000395, acc.: 100.00%] [G loss: 9.933550]\n",
      "23 [D loss: 0.003384, acc.: 100.00%] [G loss: 11.988425]\n",
      "24 [D loss: 0.388551, acc.: 87.50%] [G loss: 11.069576]\n",
      "25 [D loss: 0.001548, acc.: 100.00%] [G loss: 8.753746]\n",
      "26 [D loss: 0.257668, acc.: 84.38%] [G loss: 13.383703]\n",
      "27 [D loss: 0.258929, acc.: 93.75%] [G loss: 16.037987]\n",
      "28 [D loss: 0.101901, acc.: 93.75%] [G loss: 16.118095]\n",
      "29 [D loss: 0.007322, acc.: 100.00%] [G loss: 15.469736]\n",
      "30 [D loss: 0.853950, acc.: 59.38%] [G loss: 16.093992]\n",
      "31 [D loss: 0.001953, acc.: 100.00%] [G loss: 15.872305]\n",
      "32 [D loss: 0.004247, acc.: 100.00%] [G loss: 12.908897]\n",
      "33 [D loss: 0.007305, acc.: 100.00%] [G loss: 14.689716]\n",
      "34 [D loss: 0.017417, acc.: 100.00%] [G loss: 8.692266]\n",
      "35 [D loss: 0.044628, acc.: 100.00%] [G loss: 9.707338]\n",
      "36 [D loss: 0.280813, acc.: 84.38%] [G loss: 3.979331]\n",
      "37 [D loss: 0.001454, acc.: 100.00%] [G loss: 16.078653]\n",
      "38 [D loss: 0.511854, acc.: 78.12%] [G loss: 8.851231]\n",
      "39 [D loss: 0.340635, acc.: 90.62%] [G loss: 15.166574]\n",
      "40 [D loss: 0.008094, acc.: 100.00%] [G loss: 15.269209]\n",
      "41 [D loss: 0.006347, acc.: 100.00%] [G loss: 14.614018]\n",
      "42 [D loss: 0.359198, acc.: 75.00%] [G loss: 7.959350]\n",
      "43 [D loss: 0.256155, acc.: 87.50%] [G loss: 6.876318]\n",
      "44 [D loss: 0.019353, acc.: 100.00%] [G loss: 8.255857]\n",
      "45 [D loss: 0.636846, acc.: 65.62%] [G loss: 6.728091]\n",
      "46 [D loss: 0.103108, acc.: 93.75%] [G loss: 12.105957]\n",
      "47 [D loss: 0.008690, acc.: 100.00%] [G loss: 8.035019]\n",
      "48 [D loss: 0.007603, acc.: 100.00%] [G loss: 12.264523]\n",
      "49 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.231980]\n",
      "50 [D loss: 0.000845, acc.: 100.00%] [G loss: 12.563107]\n",
      "51 [D loss: 0.184356, acc.: 96.88%] [G loss: 15.954084]\n",
      "52 [D loss: 2.643683, acc.: 56.25%] [G loss: 4.691006]\n",
      "53 [D loss: 0.012715, acc.: 100.00%] [G loss: 1.534408]\n",
      "54 [D loss: 1.270807, acc.: 62.50%] [G loss: 0.144604]\n",
      "55 [D loss: 1.177269, acc.: 56.25%] [G loss: 3.621142]\n",
      "56 [D loss: 0.035393, acc.: 96.88%] [G loss: 7.863506]\n",
      "57 [D loss: 0.052431, acc.: 96.88%] [G loss: 9.756907]\n",
      "58 [D loss: 0.145445, acc.: 96.88%] [G loss: 11.215711]\n",
      "59 [D loss: 0.871425, acc.: 59.38%] [G loss: 10.418914]\n",
      "60 [D loss: 0.534225, acc.: 71.88%] [G loss: 8.113640]\n",
      "61 [D loss: 0.023273, acc.: 100.00%] [G loss: 6.964937]\n",
      "62 [D loss: 0.005822, acc.: 100.00%] [G loss: 4.324576]\n",
      "63 [D loss: 0.028249, acc.: 100.00%] [G loss: 5.021685]\n",
      "64 [D loss: 0.066873, acc.: 100.00%] [G loss: 5.006721]\n",
      "65 [D loss: 0.108391, acc.: 90.62%] [G loss: 4.263000]\n",
      "66 [D loss: 0.013667, acc.: 100.00%] [G loss: 3.158049]\n",
      "67 [D loss: 0.324564, acc.: 78.12%] [G loss: 4.466568]\n",
      "68 [D loss: 0.039064, acc.: 100.00%] [G loss: 4.458111]\n",
      "69 [D loss: 0.034058, acc.: 100.00%] [G loss: 4.899642]\n",
      "70 [D loss: 0.001092, acc.: 100.00%] [G loss: 3.465194]\n",
      "71 [D loss: 0.005123, acc.: 100.00%] [G loss: 5.815224]\n",
      "72 [D loss: 0.027386, acc.: 100.00%] [G loss: 5.800946]\n",
      "73 [D loss: 0.034598, acc.: 100.00%] [G loss: 7.250230]\n",
      "74 [D loss: 0.045021, acc.: 100.00%] [G loss: 6.689631]\n",
      "75 [D loss: 0.079872, acc.: 96.88%] [G loss: 3.918665]\n",
      "76 [D loss: 0.012600, acc.: 100.00%] [G loss: 2.946481]\n",
      "77 [D loss: 0.005567, acc.: 100.00%] [G loss: 6.657213]\n",
      "78 [D loss: 0.009039, acc.: 100.00%] [G loss: 7.325779]\n",
      "79 [D loss: 0.000482, acc.: 100.00%] [G loss: 5.046297]\n",
      "80 [D loss: 0.007078, acc.: 100.00%] [G loss: 2.186009]\n",
      "81 [D loss: 0.050139, acc.: 100.00%] [G loss: 2.859673]\n",
      "82 [D loss: 0.002452, acc.: 100.00%] [G loss: 2.231156]\n",
      "83 [D loss: 0.014476, acc.: 100.00%] [G loss: 1.458471]\n",
      "84 [D loss: 0.011405, acc.: 100.00%] [G loss: 4.451822]\n",
      "85 [D loss: 0.010252, acc.: 100.00%] [G loss: 2.525359]\n",
      "86 [D loss: 0.000700, acc.: 100.00%] [G loss: 2.157113]\n",
      "87 [D loss: 0.020311, acc.: 100.00%] [G loss: 2.218563]\n",
      "88 [D loss: 0.005197, acc.: 100.00%] [G loss: 2.317107]\n",
      "89 [D loss: 0.022771, acc.: 100.00%] [G loss: 1.625258]\n",
      "90 [D loss: 0.056241, acc.: 100.00%] [G loss: 2.882209]\n",
      "91 [D loss: 0.001411, acc.: 100.00%] [G loss: 1.827689]\n",
      "92 [D loss: 0.031787, acc.: 100.00%] [G loss: 0.814947]\n",
      "93 [D loss: 0.008044, acc.: 100.00%] [G loss: 2.279127]\n",
      "94 [D loss: 0.002431, acc.: 100.00%] [G loss: 2.049543]\n",
      "95 [D loss: 0.007081, acc.: 100.00%] [G loss: 4.730584]\n",
      "96 [D loss: 0.038531, acc.: 100.00%] [G loss: 3.358274]\n",
      "97 [D loss: 0.005866, acc.: 100.00%] [G loss: 4.774414]\n",
      "98 [D loss: 0.102754, acc.: 96.88%] [G loss: 1.868096]\n",
      "99 [D loss: 0.015231, acc.: 100.00%] [G loss: 4.017711]\n",
      "100 [D loss: 0.007506, acc.: 100.00%] [G loss: 3.265783]\n",
      "101 [D loss: 0.019861, acc.: 100.00%] [G loss: 5.552589]\n",
      "102 [D loss: 0.038959, acc.: 100.00%] [G loss: 1.440116]\n",
      "103 [D loss: 0.003268, acc.: 100.00%] [G loss: 3.942439]\n",
      "104 [D loss: 0.176765, acc.: 90.62%] [G loss: 4.147361]\n",
      "105 [D loss: 0.012832, acc.: 100.00%] [G loss: 6.308130]\n",
      "106 [D loss: 0.043488, acc.: 96.88%] [G loss: 2.292778]\n",
      "107 [D loss: 0.003135, acc.: 100.00%] [G loss: 1.479102]\n",
      "108 [D loss: 0.092546, acc.: 96.88%] [G loss: 0.492472]\n",
      "109 [D loss: 0.655581, acc.: 62.50%] [G loss: 2.874669]\n",
      "110 [D loss: 0.009925, acc.: 100.00%] [G loss: 8.285160]\n",
      "111 [D loss: 0.016242, acc.: 100.00%] [G loss: 8.899517]\n",
      "112 [D loss: 0.025535, acc.: 100.00%] [G loss: 12.712038]\n",
      "113 [D loss: 0.113457, acc.: 96.88%] [G loss: 12.033913]\n",
      "114 [D loss: 1.109410, acc.: 65.62%] [G loss: 4.980247]\n",
      "115 [D loss: 0.103453, acc.: 93.75%] [G loss: 6.041848]\n",
      "116 [D loss: 0.570316, acc.: 78.12%] [G loss: 3.122959]\n",
      "117 [D loss: 0.016408, acc.: 100.00%] [G loss: 2.810604]\n",
      "118 [D loss: 0.061682, acc.: 100.00%] [G loss: 4.003529]\n",
      "119 [D loss: 0.008289, acc.: 100.00%] [G loss: 6.632239]\n",
      "120 [D loss: 0.251228, acc.: 87.50%] [G loss: 5.502323]\n",
      "121 [D loss: 0.027418, acc.: 100.00%] [G loss: 5.214291]\n",
      "122 [D loss: 0.071864, acc.: 96.88%] [G loss: 6.610165]\n",
      "123 [D loss: 0.378668, acc.: 78.12%] [G loss: 5.309420]\n",
      "124 [D loss: 0.021436, acc.: 100.00%] [G loss: 4.526728]\n",
      "125 [D loss: 0.099205, acc.: 100.00%] [G loss: 4.672359]\n",
      "126 [D loss: 0.211193, acc.: 90.62%] [G loss: 6.675884]\n",
      "127 [D loss: 0.009662, acc.: 100.00%] [G loss: 5.909026]\n",
      "128 [D loss: 0.010236, acc.: 100.00%] [G loss: 4.156713]\n",
      "129 [D loss: 0.002767, acc.: 100.00%] [G loss: 2.747843]\n",
      "130 [D loss: 0.001174, acc.: 100.00%] [G loss: 7.834114]\n",
      "131 [D loss: 0.614404, acc.: 78.12%] [G loss: 6.403865]\n",
      "132 [D loss: 0.016836, acc.: 100.00%] [G loss: 2.378623]\n",
      "133 [D loss: 0.049072, acc.: 96.88%] [G loss: 1.626508]\n",
      "134 [D loss: 0.015053, acc.: 100.00%] [G loss: 2.653079]\n",
      "135 [D loss: 0.087392, acc.: 96.88%] [G loss: 1.336417]\n",
      "136 [D loss: 0.239267, acc.: 87.50%] [G loss: 1.604024]\n",
      "137 [D loss: 0.799300, acc.: 68.75%] [G loss: 3.209509]\n",
      "138 [D loss: 0.336111, acc.: 84.38%] [G loss: 7.973212]\n",
      "139 [D loss: 0.128105, acc.: 96.88%] [G loss: 7.837185]\n",
      "140 [D loss: 0.065676, acc.: 100.00%] [G loss: 8.240237]\n",
      "141 [D loss: 0.694987, acc.: 59.38%] [G loss: 7.521598]\n",
      "142 [D loss: 0.114046, acc.: 96.88%] [G loss: 5.820466]\n",
      "143 [D loss: 0.384446, acc.: 81.25%] [G loss: 4.337367]\n",
      "144 [D loss: 0.201985, acc.: 87.50%] [G loss: 1.793088]\n",
      "145 [D loss: 1.368188, acc.: 56.25%] [G loss: 3.960634]\n",
      "146 [D loss: 0.060589, acc.: 100.00%] [G loss: 5.477043]\n",
      "147 [D loss: 0.081027, acc.: 96.88%] [G loss: 5.177128]\n",
      "148 [D loss: 0.007858, acc.: 100.00%] [G loss: 8.880880]\n",
      "149 [D loss: 0.376934, acc.: 78.12%] [G loss: 7.303984]\n",
      "150 [D loss: 0.495118, acc.: 84.38%] [G loss: 5.170454]\n",
      "151 [D loss: 0.174281, acc.: 93.75%] [G loss: 7.216404]\n",
      "152 [D loss: 0.105118, acc.: 96.88%] [G loss: 2.719270]\n",
      "153 [D loss: 0.419245, acc.: 84.38%] [G loss: 4.986945]\n",
      "154 [D loss: 0.038572, acc.: 100.00%] [G loss: 6.337257]\n",
      "155 [D loss: 0.148403, acc.: 93.75%] [G loss: 5.278810]\n",
      "156 [D loss: 1.351924, acc.: 50.00%] [G loss: 3.354198]\n",
      "157 [D loss: 0.063032, acc.: 100.00%] [G loss: 3.637362]\n",
      "158 [D loss: 0.157214, acc.: 90.62%] [G loss: 1.480039]\n",
      "159 [D loss: 0.782230, acc.: 65.62%] [G loss: 2.284636]\n",
      "160 [D loss: 0.436043, acc.: 84.38%] [G loss: 0.423118]\n",
      "161 [D loss: 0.338826, acc.: 87.50%] [G loss: 5.516009]\n",
      "162 [D loss: 0.187522, acc.: 96.88%] [G loss: 6.323874]\n",
      "163 [D loss: 0.108028, acc.: 96.88%] [G loss: 9.340985]\n",
      "164 [D loss: 0.303936, acc.: 90.62%] [G loss: 10.927010]\n",
      "165 [D loss: 0.429711, acc.: 81.25%] [G loss: 6.975304]\n",
      "166 [D loss: 0.324928, acc.: 84.38%] [G loss: 5.800984]\n",
      "167 [D loss: 0.085832, acc.: 96.88%] [G loss: 3.137912]\n",
      "168 [D loss: 1.803746, acc.: 53.12%] [G loss: 3.238123]\n",
      "169 [D loss: 0.545863, acc.: 84.38%] [G loss: 6.861004]\n",
      "170 [D loss: 0.367279, acc.: 81.25%] [G loss: 7.261100]\n",
      "171 [D loss: 0.218726, acc.: 90.62%] [G loss: 9.725733]\n",
      "172 [D loss: 0.004517, acc.: 100.00%] [G loss: 6.857050]\n",
      "173 [D loss: 0.344855, acc.: 84.38%] [G loss: 7.598527]\n",
      "174 [D loss: 0.263974, acc.: 87.50%] [G loss: 7.467176]\n",
      "175 [D loss: 2.312118, acc.: 37.50%] [G loss: 7.701415]\n",
      "176 [D loss: 0.037033, acc.: 100.00%] [G loss: 7.458985]\n",
      "177 [D loss: 0.053528, acc.: 100.00%] [G loss: 2.771492]\n",
      "178 [D loss: 0.966897, acc.: 56.25%] [G loss: 4.810302]\n",
      "179 [D loss: 0.132964, acc.: 93.75%] [G loss: 2.805060]\n",
      "180 [D loss: 0.237850, acc.: 90.62%] [G loss: 5.496517]\n",
      "181 [D loss: 0.061898, acc.: 100.00%] [G loss: 2.486382]\n",
      "182 [D loss: 0.024495, acc.: 100.00%] [G loss: 6.446638]\n",
      "183 [D loss: 0.219815, acc.: 87.50%] [G loss: 6.249866]\n",
      "184 [D loss: 0.023901, acc.: 100.00%] [G loss: 2.968605]\n",
      "185 [D loss: 0.154975, acc.: 100.00%] [G loss: 2.471598]\n",
      "186 [D loss: 0.055094, acc.: 100.00%] [G loss: 2.007742]\n",
      "187 [D loss: 0.354337, acc.: 81.25%] [G loss: 1.255852]\n",
      "188 [D loss: 0.117477, acc.: 96.88%] [G loss: 2.278309]\n",
      "189 [D loss: 0.605423, acc.: 71.88%] [G loss: 0.862323]\n",
      "190 [D loss: 0.068760, acc.: 100.00%] [G loss: 1.090323]\n",
      "191 [D loss: 0.301865, acc.: 71.88%] [G loss: 0.763013]\n",
      "192 [D loss: 0.396988, acc.: 84.38%] [G loss: 1.061437]\n",
      "193 [D loss: 0.073235, acc.: 96.88%] [G loss: 2.161289]\n",
      "194 [D loss: 0.151181, acc.: 93.75%] [G loss: 4.104318]\n",
      "195 [D loss: 0.038465, acc.: 100.00%] [G loss: 3.833532]\n",
      "196 [D loss: 0.180920, acc.: 90.62%] [G loss: 2.060905]\n",
      "197 [D loss: 0.063090, acc.: 100.00%] [G loss: 1.909102]\n",
      "198 [D loss: 0.397409, acc.: 75.00%] [G loss: 1.686946]\n",
      "199 [D loss: 0.161540, acc.: 90.62%] [G loss: 0.216418]\n",
      "200 [D loss: 0.924091, acc.: 50.00%] [G loss: 0.687487]\n",
      "201 [D loss: 0.154686, acc.: 93.75%] [G loss: 0.040362]\n",
      "202 [D loss: 0.287435, acc.: 84.38%] [G loss: 0.893171]\n",
      "203 [D loss: 0.239500, acc.: 87.50%] [G loss: 4.782146]\n",
      "204 [D loss: 0.024342, acc.: 100.00%] [G loss: 7.775989]\n",
      "205 [D loss: 0.064833, acc.: 100.00%] [G loss: 5.604137]\n",
      "206 [D loss: 0.260923, acc.: 87.50%] [G loss: 4.796535]\n",
      "207 [D loss: 0.319548, acc.: 90.62%] [G loss: 3.081312]\n",
      "208 [D loss: 0.231593, acc.: 87.50%] [G loss: 2.209333]\n",
      "209 [D loss: 0.447138, acc.: 68.75%] [G loss: 1.417489]\n",
      "210 [D loss: 0.920119, acc.: 59.38%] [G loss: 3.798554]\n",
      "211 [D loss: 0.191545, acc.: 93.75%] [G loss: 5.458695]\n",
      "212 [D loss: 0.702870, acc.: 68.75%] [G loss: 6.227091]\n",
      "213 [D loss: 0.125434, acc.: 96.88%] [G loss: 5.255544]\n",
      "214 [D loss: 0.092058, acc.: 96.88%] [G loss: 4.271869]\n",
      "215 [D loss: 0.378993, acc.: 87.50%] [G loss: 3.918057]\n",
      "216 [D loss: 0.047675, acc.: 96.88%] [G loss: 2.565880]\n",
      "217 [D loss: 0.212049, acc.: 87.50%] [G loss: 3.219138]\n",
      "218 [D loss: 0.128935, acc.: 96.88%] [G loss: 3.934388]\n",
      "219 [D loss: 0.626804, acc.: 65.62%] [G loss: 1.494004]\n",
      "220 [D loss: 0.007292, acc.: 100.00%] [G loss: 3.187423]\n",
      "221 [D loss: 0.190055, acc.: 90.62%] [G loss: 2.959179]\n",
      "222 [D loss: 0.049990, acc.: 100.00%] [G loss: 2.551228]\n",
      "223 [D loss: 0.105320, acc.: 93.75%] [G loss: 3.667572]\n",
      "224 [D loss: 0.283358, acc.: 84.38%] [G loss: 3.233203]\n",
      "225 [D loss: 0.396100, acc.: 81.25%] [G loss: 5.270074]\n",
      "226 [D loss: 0.058705, acc.: 96.88%] [G loss: 5.665029]\n",
      "227 [D loss: 1.817492, acc.: 56.25%] [G loss: 3.404742]\n",
      "228 [D loss: 0.063943, acc.: 100.00%] [G loss: 2.868009]\n",
      "229 [D loss: 0.169791, acc.: 90.62%] [G loss: 2.028723]\n",
      "230 [D loss: 0.257151, acc.: 81.25%] [G loss: 2.584487]\n",
      "231 [D loss: 0.255435, acc.: 90.62%] [G loss: 2.066626]\n",
      "232 [D loss: 0.094958, acc.: 100.00%] [G loss: 3.243118]\n",
      "233 [D loss: 0.366455, acc.: 81.25%] [G loss: 2.540121]\n",
      "234 [D loss: 0.321458, acc.: 84.38%] [G loss: 4.821572]\n",
      "235 [D loss: 0.133194, acc.: 96.88%] [G loss: 2.274381]\n",
      "236 [D loss: 0.378191, acc.: 75.00%] [G loss: 3.944787]\n",
      "237 [D loss: 0.207787, acc.: 93.75%] [G loss: 3.745003]\n",
      "238 [D loss: 0.366103, acc.: 84.38%] [G loss: 6.849858]\n",
      "239 [D loss: 0.261607, acc.: 90.62%] [G loss: 6.431919]\n",
      "240 [D loss: 0.156919, acc.: 96.88%] [G loss: 4.605495]\n",
      "241 [D loss: 0.285724, acc.: 87.50%] [G loss: 7.061498]\n",
      "242 [D loss: 0.290485, acc.: 81.25%] [G loss: 4.888324]\n",
      "243 [D loss: 0.091032, acc.: 96.88%] [G loss: 7.728738]\n",
      "244 [D loss: 0.324239, acc.: 78.12%] [G loss: 5.662278]\n",
      "245 [D loss: 0.020960, acc.: 100.00%] [G loss: 8.346830]\n",
      "246 [D loss: 0.109345, acc.: 96.88%] [G loss: 8.694948]\n",
      "247 [D loss: 0.033670, acc.: 100.00%] [G loss: 8.804417]\n",
      "248 [D loss: 0.701140, acc.: 65.62%] [G loss: 5.369822]\n",
      "249 [D loss: 0.067178, acc.: 100.00%] [G loss: 2.741980]\n",
      "250 [D loss: 0.185816, acc.: 96.88%] [G loss: 4.496894]\n",
      "251 [D loss: 0.177355, acc.: 93.75%] [G loss: 2.221494]\n",
      "252 [D loss: 0.143521, acc.: 96.88%] [G loss: 3.722857]\n",
      "253 [D loss: 0.281304, acc.: 90.62%] [G loss: 4.754409]\n",
      "254 [D loss: 0.074346, acc.: 96.88%] [G loss: 8.449860]\n",
      "255 [D loss: 0.128007, acc.: 93.75%] [G loss: 10.477583]\n",
      "256 [D loss: 0.100353, acc.: 96.88%] [G loss: 7.529455]\n",
      "257 [D loss: 0.017382, acc.: 100.00%] [G loss: 7.115140]\n",
      "258 [D loss: 0.280684, acc.: 87.50%] [G loss: 6.772392]\n",
      "259 [D loss: 0.312131, acc.: 90.62%] [G loss: 9.918103]\n",
      "260 [D loss: 0.073925, acc.: 96.88%] [G loss: 3.741544]\n",
      "261 [D loss: 0.120032, acc.: 96.88%] [G loss: 2.105114]\n",
      "262 [D loss: 0.029249, acc.: 100.00%] [G loss: 0.284027]\n",
      "263 [D loss: 2.449023, acc.: 53.12%] [G loss: 1.969519]\n",
      "264 [D loss: 0.460932, acc.: 71.88%] [G loss: 2.739374]\n",
      "265 [D loss: 0.382676, acc.: 93.75%] [G loss: 3.640395]\n",
      "266 [D loss: 0.720839, acc.: 75.00%] [G loss: 5.183899]\n",
      "267 [D loss: 0.433821, acc.: 81.25%] [G loss: 4.677310]\n",
      "268 [D loss: 0.494772, acc.: 84.38%] [G loss: 4.435599]\n",
      "269 [D loss: 0.099913, acc.: 100.00%] [G loss: 3.101419]\n",
      "270 [D loss: 0.129441, acc.: 96.88%] [G loss: 3.821783]\n",
      "271 [D loss: 0.364306, acc.: 87.50%] [G loss: 5.148554]\n",
      "272 [D loss: 0.086282, acc.: 96.88%] [G loss: 3.936876]\n",
      "273 [D loss: 0.355313, acc.: 81.25%] [G loss: 5.772567]\n",
      "274 [D loss: 0.762446, acc.: 56.25%] [G loss: 4.443775]\n",
      "275 [D loss: 0.059773, acc.: 100.00%] [G loss: 5.060460]\n",
      "276 [D loss: 0.036619, acc.: 100.00%] [G loss: 4.925465]\n",
      "277 [D loss: 0.268168, acc.: 87.50%] [G loss: 2.852644]\n",
      "278 [D loss: 0.054718, acc.: 96.88%] [G loss: 2.880939]\n",
      "279 [D loss: 0.300287, acc.: 90.62%] [G loss: 1.343895]\n",
      "280 [D loss: 0.274235, acc.: 84.38%] [G loss: 2.632478]\n",
      "281 [D loss: 0.485795, acc.: 75.00%] [G loss: 4.712978]\n",
      "282 [D loss: 0.146179, acc.: 90.62%] [G loss: 6.200684]\n",
      "283 [D loss: 0.181235, acc.: 93.75%] [G loss: 6.241540]\n",
      "284 [D loss: 0.156216, acc.: 96.88%] [G loss: 9.397596]\n",
      "285 [D loss: 1.089782, acc.: 59.38%] [G loss: 4.453130]\n",
      "286 [D loss: 0.537588, acc.: 81.25%] [G loss: 5.349329]\n",
      "287 [D loss: 0.029607, acc.: 100.00%] [G loss: 0.905569]\n",
      "288 [D loss: 1.190189, acc.: 56.25%] [G loss: 1.246832]\n",
      "289 [D loss: 0.035916, acc.: 100.00%] [G loss: 2.084806]\n",
      "290 [D loss: 0.135437, acc.: 93.75%] [G loss: 4.826000]\n",
      "291 [D loss: 0.534892, acc.: 84.38%] [G loss: 4.801666]\n",
      "292 [D loss: 0.228263, acc.: 93.75%] [G loss: 4.835535]\n",
      "293 [D loss: 0.233391, acc.: 90.62%] [G loss: 5.107558]\n",
      "294 [D loss: 1.102400, acc.: 62.50%] [G loss: 2.393869]\n",
      "295 [D loss: 0.355324, acc.: 90.62%] [G loss: 2.468257]\n",
      "296 [D loss: 0.684721, acc.: 62.50%] [G loss: 1.701290]\n",
      "297 [D loss: 0.382692, acc.: 84.38%] [G loss: 1.953542]\n",
      "298 [D loss: 0.162945, acc.: 96.88%] [G loss: 4.870246]\n",
      "299 [D loss: 0.538013, acc.: 84.38%] [G loss: 4.247370]\n",
      "300 [D loss: 0.226144, acc.: 87.50%] [G loss: 5.389094]\n",
      "301 [D loss: 0.143059, acc.: 96.88%] [G loss: 4.023113]\n",
      "302 [D loss: 0.110502, acc.: 100.00%] [G loss: 4.148710]\n",
      "303 [D loss: 0.252131, acc.: 87.50%] [G loss: 3.079425]\n",
      "304 [D loss: 0.355559, acc.: 84.38%] [G loss: 2.802420]\n",
      "305 [D loss: 0.158133, acc.: 93.75%] [G loss: 2.884536]\n",
      "306 [D loss: 0.232157, acc.: 93.75%] [G loss: 2.813864]\n",
      "307 [D loss: 0.186154, acc.: 93.75%] [G loss: 3.350524]\n",
      "308 [D loss: 0.160025, acc.: 93.75%] [G loss: 2.156623]\n",
      "309 [D loss: 0.099402, acc.: 96.88%] [G loss: 4.093828]\n",
      "310 [D loss: 0.052377, acc.: 100.00%] [G loss: 3.544785]\n",
      "311 [D loss: 0.258984, acc.: 90.62%] [G loss: 5.486835]\n",
      "312 [D loss: 0.442375, acc.: 78.12%] [G loss: 4.047622]\n",
      "313 [D loss: 0.366962, acc.: 84.38%] [G loss: 4.699430]\n",
      "314 [D loss: 0.064495, acc.: 100.00%] [G loss: 4.621286]\n",
      "315 [D loss: 0.082352, acc.: 100.00%] [G loss: 6.529004]\n",
      "316 [D loss: 0.102112, acc.: 96.88%] [G loss: 1.874465]\n",
      "317 [D loss: 0.418839, acc.: 84.38%] [G loss: 3.861892]\n",
      "318 [D loss: 0.213385, acc.: 93.75%] [G loss: 3.473216]\n",
      "319 [D loss: 0.131870, acc.: 90.62%] [G loss: 1.764495]\n",
      "320 [D loss: 0.575280, acc.: 81.25%] [G loss: 2.869140]\n",
      "321 [D loss: 0.794680, acc.: 75.00%] [G loss: 1.724403]\n",
      "322 [D loss: 0.113850, acc.: 93.75%] [G loss: 5.975350]\n",
      "323 [D loss: 0.218916, acc.: 87.50%] [G loss: 7.317653]\n",
      "324 [D loss: 0.188267, acc.: 90.62%] [G loss: 5.637129]\n",
      "325 [D loss: 0.283016, acc.: 90.62%] [G loss: 3.647888]\n",
      "326 [D loss: 0.171367, acc.: 90.62%] [G loss: 4.013638]\n",
      "327 [D loss: 0.266640, acc.: 90.62%] [G loss: 2.166255]\n",
      "328 [D loss: 0.168362, acc.: 93.75%] [G loss: 0.481443]\n",
      "329 [D loss: 0.637568, acc.: 62.50%] [G loss: 0.410461]\n",
      "330 [D loss: 0.291363, acc.: 90.62%] [G loss: 2.398046]\n",
      "331 [D loss: 0.155153, acc.: 96.88%] [G loss: 2.236906]\n",
      "332 [D loss: 0.195843, acc.: 93.75%] [G loss: 5.073071]\n",
      "333 [D loss: 0.346526, acc.: 81.25%] [G loss: 3.549160]\n",
      "334 [D loss: 0.022718, acc.: 100.00%] [G loss: 4.383117]\n",
      "335 [D loss: 0.136777, acc.: 93.75%] [G loss: 2.106238]\n",
      "336 [D loss: 0.148895, acc.: 96.88%] [G loss: 2.904983]\n",
      "337 [D loss: 0.373222, acc.: 81.25%] [G loss: 2.807682]\n",
      "338 [D loss: 0.170041, acc.: 90.62%] [G loss: 2.540734]\n",
      "339 [D loss: 0.148860, acc.: 90.62%] [G loss: 4.013593]\n",
      "340 [D loss: 0.217109, acc.: 93.75%] [G loss: 4.174585]\n",
      "341 [D loss: 0.604171, acc.: 78.12%] [G loss: 3.966457]\n",
      "342 [D loss: 0.161153, acc.: 96.88%] [G loss: 5.651624]\n",
      "343 [D loss: 0.099622, acc.: 93.75%] [G loss: 3.720014]\n",
      "344 [D loss: 0.068821, acc.: 96.88%] [G loss: 5.744078]\n",
      "345 [D loss: 0.025079, acc.: 100.00%] [G loss: 4.371229]\n",
      "346 [D loss: 0.055941, acc.: 100.00%] [G loss: 3.539719]\n",
      "347 [D loss: 0.137520, acc.: 100.00%] [G loss: 3.704277]\n",
      "348 [D loss: 0.085556, acc.: 100.00%] [G loss: 4.744568]\n",
      "349 [D loss: 0.411317, acc.: 75.00%] [G loss: 2.952603]\n",
      "350 [D loss: 0.493324, acc.: 75.00%] [G loss: 1.356966]\n",
      "351 [D loss: 0.085405, acc.: 96.88%] [G loss: 1.339086]\n",
      "352 [D loss: 1.307607, acc.: 56.25%] [G loss: 2.024778]\n",
      "353 [D loss: 0.026504, acc.: 100.00%] [G loss: 4.663318]\n",
      "354 [D loss: 0.141561, acc.: 93.75%] [G loss: 10.352458]\n",
      "355 [D loss: 0.736559, acc.: 71.88%] [G loss: 6.370486]\n",
      "356 [D loss: 0.497047, acc.: 81.25%] [G loss: 7.220728]\n",
      "357 [D loss: 0.065235, acc.: 100.00%] [G loss: 2.439569]\n",
      "358 [D loss: 0.595447, acc.: 71.88%] [G loss: 2.878791]\n",
      "359 [D loss: 0.135329, acc.: 96.88%] [G loss: 3.923804]\n",
      "360 [D loss: 0.025999, acc.: 100.00%] [G loss: 3.634483]\n",
      "361 [D loss: 0.197974, acc.: 93.75%] [G loss: 5.218951]\n",
      "362 [D loss: 0.141722, acc.: 93.75%] [G loss: 4.663082]\n",
      "363 [D loss: 0.049907, acc.: 100.00%] [G loss: 6.587752]\n",
      "364 [D loss: 0.073827, acc.: 100.00%] [G loss: 4.751393]\n",
      "365 [D loss: 0.197932, acc.: 93.75%] [G loss: 3.846329]\n",
      "366 [D loss: 0.118171, acc.: 96.88%] [G loss: 5.399175]\n",
      "367 [D loss: 0.037196, acc.: 100.00%] [G loss: 5.677540]\n",
      "368 [D loss: 0.099005, acc.: 96.88%] [G loss: 2.807455]\n",
      "369 [D loss: 0.141943, acc.: 93.75%] [G loss: 4.027810]\n",
      "370 [D loss: 0.081327, acc.: 96.88%] [G loss: 2.671159]\n",
      "371 [D loss: 0.181523, acc.: 90.62%] [G loss: 1.447623]\n",
      "372 [D loss: 0.255489, acc.: 84.38%] [G loss: 1.664642]\n",
      "373 [D loss: 0.042261, acc.: 100.00%] [G loss: 2.034702]\n",
      "374 [D loss: 0.746985, acc.: 59.38%] [G loss: 4.075903]\n",
      "375 [D loss: 0.206143, acc.: 90.62%] [G loss: 7.288942]\n",
      "376 [D loss: 0.371567, acc.: 84.38%] [G loss: 7.486596]\n",
      "377 [D loss: 0.538956, acc.: 71.88%] [G loss: 4.649388]\n",
      "378 [D loss: 0.406622, acc.: 78.12%] [G loss: 2.232869]\n",
      "379 [D loss: 0.393979, acc.: 84.38%] [G loss: 0.693788]\n",
      "380 [D loss: 0.809481, acc.: 65.62%] [G loss: 2.806116]\n",
      "381 [D loss: 0.016064, acc.: 100.00%] [G loss: 2.913253]\n",
      "382 [D loss: 0.229035, acc.: 96.88%] [G loss: 3.688082]\n",
      "383 [D loss: 0.383101, acc.: 81.25%] [G loss: 2.498997]\n",
      "384 [D loss: 0.061671, acc.: 100.00%] [G loss: 4.628716]\n",
      "385 [D loss: 0.013908, acc.: 100.00%] [G loss: 5.958836]\n",
      "386 [D loss: 0.274205, acc.: 87.50%] [G loss: 6.507351]\n",
      "387 [D loss: 0.065299, acc.: 100.00%] [G loss: 4.730990]\n",
      "388 [D loss: 0.049854, acc.: 96.88%] [G loss: 1.936082]\n",
      "389 [D loss: 0.195824, acc.: 93.75%] [G loss: 2.415064]\n",
      "390 [D loss: 0.083218, acc.: 96.88%] [G loss: 4.515548]\n",
      "391 [D loss: 0.131574, acc.: 96.88%] [G loss: 4.443593]\n",
      "392 [D loss: 0.106881, acc.: 90.62%] [G loss: 1.016069]\n",
      "393 [D loss: 0.081802, acc.: 100.00%] [G loss: 5.562840]\n",
      "394 [D loss: 0.683559, acc.: 75.00%] [G loss: 3.079264]\n",
      "395 [D loss: 0.202271, acc.: 90.62%] [G loss: 3.714480]\n",
      "396 [D loss: 0.227907, acc.: 87.50%] [G loss: 2.826805]\n",
      "397 [D loss: 0.101171, acc.: 96.88%] [G loss: 3.726082]\n",
      "398 [D loss: 0.523324, acc.: 81.25%] [G loss: 4.376535]\n",
      "399 [D loss: 0.025720, acc.: 100.00%] [G loss: 4.803471]\n",
      "400 [D loss: 0.144315, acc.: 96.88%] [G loss: 4.690157]\n",
      "401 [D loss: 0.403456, acc.: 75.00%] [G loss: 2.614883]\n",
      "402 [D loss: 0.104572, acc.: 96.88%] [G loss: 1.661454]\n",
      "403 [D loss: 0.129217, acc.: 93.75%] [G loss: 2.954312]\n",
      "404 [D loss: 0.297745, acc.: 90.62%] [G loss: 2.371707]\n",
      "405 [D loss: 0.066004, acc.: 96.88%] [G loss: 1.836852]\n",
      "406 [D loss: 0.256176, acc.: 84.38%] [G loss: 2.807067]\n",
      "407 [D loss: 0.477970, acc.: 81.25%] [G loss: 4.379169]\n",
      "408 [D loss: 0.217325, acc.: 90.62%] [G loss: 5.560665]\n",
      "409 [D loss: 0.216894, acc.: 87.50%] [G loss: 3.807523]\n",
      "410 [D loss: 0.144606, acc.: 93.75%] [G loss: 3.528930]\n",
      "411 [D loss: 0.030128, acc.: 100.00%] [G loss: 2.795060]\n",
      "412 [D loss: 0.290893, acc.: 78.12%] [G loss: 3.328762]\n",
      "413 [D loss: 0.228461, acc.: 90.62%] [G loss: 5.905784]\n",
      "414 [D loss: 0.066951, acc.: 96.88%] [G loss: 5.589076]\n",
      "415 [D loss: 0.708701, acc.: 68.75%] [G loss: 3.457128]\n",
      "416 [D loss: 0.681286, acc.: 68.75%] [G loss: 6.396875]\n",
      "417 [D loss: 0.137402, acc.: 96.88%] [G loss: 5.931705]\n",
      "418 [D loss: 0.317459, acc.: 84.38%] [G loss: 7.446211]\n",
      "419 [D loss: 0.189364, acc.: 93.75%] [G loss: 4.332829]\n",
      "420 [D loss: 0.020341, acc.: 100.00%] [G loss: 7.339523]\n",
      "421 [D loss: 0.559946, acc.: 75.00%] [G loss: 4.116157]\n",
      "422 [D loss: 0.192526, acc.: 90.62%] [G loss: 3.779946]\n",
      "423 [D loss: 0.539004, acc.: 68.75%] [G loss: 2.590042]\n",
      "424 [D loss: 0.748362, acc.: 78.12%] [G loss: 2.718304]\n",
      "425 [D loss: 0.075893, acc.: 96.88%] [G loss: 3.372335]\n",
      "426 [D loss: 0.047996, acc.: 100.00%] [G loss: 4.702249]\n",
      "427 [D loss: 0.076201, acc.: 96.88%] [G loss: 5.732763]\n",
      "428 [D loss: 0.187738, acc.: 93.75%] [G loss: 4.304087]\n",
      "429 [D loss: 0.201264, acc.: 87.50%] [G loss: 3.823667]\n",
      "430 [D loss: 0.094634, acc.: 96.88%] [G loss: 4.335389]\n",
      "431 [D loss: 0.277993, acc.: 96.88%] [G loss: 2.343383]\n",
      "432 [D loss: 0.054656, acc.: 100.00%] [G loss: 3.588582]\n",
      "433 [D loss: 0.205145, acc.: 93.75%] [G loss: 2.706907]\n",
      "434 [D loss: 0.082709, acc.: 100.00%] [G loss: 4.849392]\n",
      "435 [D loss: 0.038180, acc.: 100.00%] [G loss: 4.575803]\n",
      "436 [D loss: 0.417157, acc.: 81.25%] [G loss: 3.730087]\n",
      "437 [D loss: 0.145407, acc.: 96.88%] [G loss: 2.620287]\n",
      "438 [D loss: 0.102520, acc.: 93.75%] [G loss: 5.251810]\n",
      "439 [D loss: 0.165688, acc.: 93.75%] [G loss: 5.075102]\n",
      "440 [D loss: 0.054852, acc.: 96.88%] [G loss: 6.067019]\n",
      "441 [D loss: 0.029834, acc.: 100.00%] [G loss: 4.201333]\n",
      "442 [D loss: 0.360513, acc.: 87.50%] [G loss: 1.918740]\n",
      "443 [D loss: 0.263216, acc.: 81.25%] [G loss: 2.333308]\n",
      "444 [D loss: 0.159270, acc.: 96.88%] [G loss: 6.843858]\n",
      "445 [D loss: 0.120910, acc.: 90.62%] [G loss: 6.319766]\n",
      "446 [D loss: 0.144126, acc.: 96.88%] [G loss: 7.299195]\n",
      "447 [D loss: 0.579369, acc.: 81.25%] [G loss: 3.954269]\n",
      "448 [D loss: 0.094918, acc.: 93.75%] [G loss: 3.233697]\n",
      "449 [D loss: 0.166976, acc.: 93.75%] [G loss: 2.272758]\n",
      "450 [D loss: 0.228902, acc.: 87.50%] [G loss: 2.803748]\n",
      "451 [D loss: 0.104633, acc.: 96.88%] [G loss: 3.520575]\n",
      "452 [D loss: 0.033031, acc.: 100.00%] [G loss: 4.745173]\n",
      "453 [D loss: 0.208575, acc.: 87.50%] [G loss: 5.707119]\n",
      "454 [D loss: 0.004573, acc.: 100.00%] [G loss: 6.804291]\n",
      "455 [D loss: 0.081260, acc.: 96.88%] [G loss: 5.757730]\n",
      "456 [D loss: 0.380840, acc.: 84.38%] [G loss: 3.634085]\n",
      "457 [D loss: 0.170719, acc.: 90.62%] [G loss: 4.624939]\n",
      "458 [D loss: 0.047248, acc.: 96.88%] [G loss: 3.714899]\n",
      "459 [D loss: 0.066900, acc.: 100.00%] [G loss: 5.101830]\n",
      "460 [D loss: 0.341953, acc.: 81.25%] [G loss: 7.229443]\n",
      "461 [D loss: 0.224003, acc.: 87.50%] [G loss: 7.012884]\n",
      "462 [D loss: 0.023980, acc.: 100.00%] [G loss: 5.698626]\n",
      "463 [D loss: 0.227490, acc.: 90.62%] [G loss: 4.961667]\n",
      "464 [D loss: 1.096189, acc.: 65.62%] [G loss: 1.835109]\n",
      "465 [D loss: 1.810326, acc.: 53.12%] [G loss: 0.677480]\n",
      "466 [D loss: 0.078194, acc.: 96.88%] [G loss: 4.587539]\n",
      "467 [D loss: 0.049133, acc.: 100.00%] [G loss: 5.994157]\n",
      "468 [D loss: 0.020689, acc.: 100.00%] [G loss: 6.225636]\n",
      "469 [D loss: 0.111698, acc.: 93.75%] [G loss: 9.312683]\n",
      "470 [D loss: 0.272007, acc.: 87.50%] [G loss: 6.371184]\n",
      "471 [D loss: 0.163874, acc.: 93.75%] [G loss: 5.784223]\n",
      "472 [D loss: 0.232148, acc.: 93.75%] [G loss: 3.754299]\n",
      "473 [D loss: 0.318077, acc.: 81.25%] [G loss: 4.431190]\n",
      "474 [D loss: 0.485627, acc.: 78.12%] [G loss: 5.829823]\n",
      "475 [D loss: 0.242886, acc.: 93.75%] [G loss: 5.150128]\n",
      "476 [D loss: 0.156248, acc.: 96.88%] [G loss: 8.627396]\n",
      "477 [D loss: 0.940163, acc.: 62.50%] [G loss: 4.581878]\n",
      "478 [D loss: 0.245452, acc.: 87.50%] [G loss: 1.189732]\n",
      "479 [D loss: 0.592572, acc.: 68.75%] [G loss: 1.158938]\n",
      "480 [D loss: 0.130299, acc.: 93.75%] [G loss: 5.175214]\n",
      "481 [D loss: 0.036408, acc.: 100.00%] [G loss: 3.241206]\n",
      "482 [D loss: 0.243453, acc.: 90.62%] [G loss: 3.415159]\n",
      "483 [D loss: 0.663012, acc.: 65.62%] [G loss: 7.877578]\n",
      "484 [D loss: 0.218844, acc.: 84.38%] [G loss: 8.073929]\n",
      "485 [D loss: 1.249451, acc.: 59.38%] [G loss: 7.555850]\n",
      "486 [D loss: 0.092695, acc.: 96.88%] [G loss: 5.203256]\n",
      "487 [D loss: 0.172817, acc.: 90.62%] [G loss: 3.928704]\n",
      "488 [D loss: 0.145067, acc.: 90.62%] [G loss: 4.458371]\n",
      "489 [D loss: 0.113217, acc.: 93.75%] [G loss: 4.129189]\n",
      "490 [D loss: 0.253201, acc.: 87.50%] [G loss: 3.779189]\n",
      "491 [D loss: 0.400817, acc.: 93.75%] [G loss: 4.847116]\n",
      "492 [D loss: 0.136439, acc.: 96.88%] [G loss: 4.020211]\n",
      "493 [D loss: 0.319465, acc.: 96.88%] [G loss: 6.018644]\n",
      "494 [D loss: 0.053789, acc.: 100.00%] [G loss: 3.601223]\n",
      "495 [D loss: 0.293483, acc.: 81.25%] [G loss: 3.783645]\n",
      "496 [D loss: 0.068254, acc.: 100.00%] [G loss: 3.913694]\n",
      "497 [D loss: 0.153260, acc.: 90.62%] [G loss: 1.790204]\n",
      "498 [D loss: 0.073997, acc.: 96.88%] [G loss: 0.970090]\n",
      "499 [D loss: 0.115815, acc.: 93.75%] [G loss: 3.298300]\n",
      "500 [D loss: 0.577991, acc.: 78.12%] [G loss: 3.580004]\n",
      "501 [D loss: 0.019227, acc.: 100.00%] [G loss: 4.982203]\n",
      "502 [D loss: 0.110066, acc.: 96.88%] [G loss: 3.490092]\n",
      "503 [D loss: 0.192260, acc.: 90.62%] [G loss: 7.425973]\n",
      "504 [D loss: 0.169387, acc.: 96.88%] [G loss: 7.492927]\n",
      "505 [D loss: 0.668064, acc.: 65.62%] [G loss: 3.537353]\n",
      "506 [D loss: 0.007016, acc.: 100.00%] [G loss: 3.572303]\n",
      "507 [D loss: 1.325297, acc.: 53.12%] [G loss: 0.841367]\n",
      "508 [D loss: 0.021733, acc.: 100.00%] [G loss: 1.310339]\n",
      "509 [D loss: 0.490358, acc.: 81.25%] [G loss: 6.056869]\n",
      "510 [D loss: 0.328690, acc.: 87.50%] [G loss: 4.196708]\n",
      "511 [D loss: 0.078574, acc.: 96.88%] [G loss: 5.611526]\n",
      "512 [D loss: 0.264055, acc.: 87.50%] [G loss: 8.598235]\n",
      "513 [D loss: 0.377079, acc.: 78.12%] [G loss: 5.506014]\n",
      "514 [D loss: 0.035110, acc.: 100.00%] [G loss: 2.917585]\n",
      "515 [D loss: 0.121232, acc.: 90.62%] [G loss: 3.084475]\n",
      "516 [D loss: 0.251236, acc.: 87.50%] [G loss: 4.235046]\n",
      "517 [D loss: 0.203216, acc.: 93.75%] [G loss: 3.501452]\n",
      "518 [D loss: 0.284345, acc.: 84.38%] [G loss: 3.924180]\n",
      "519 [D loss: 0.238178, acc.: 87.50%] [G loss: 2.926496]\n",
      "520 [D loss: 0.177357, acc.: 93.75%] [G loss: 6.245414]\n",
      "521 [D loss: 0.147771, acc.: 90.62%] [G loss: 4.686829]\n",
      "522 [D loss: 0.051567, acc.: 100.00%] [G loss: 4.970244]\n",
      "523 [D loss: 0.344891, acc.: 87.50%] [G loss: 6.118014]\n",
      "524 [D loss: 0.203383, acc.: 90.62%] [G loss: 3.500680]\n",
      "525 [D loss: 0.344065, acc.: 87.50%] [G loss: 3.226893]\n",
      "526 [D loss: 0.182010, acc.: 93.75%] [G loss: 3.223186]\n",
      "527 [D loss: 0.320224, acc.: 78.12%] [G loss: 4.268286]\n",
      "528 [D loss: 0.126762, acc.: 96.88%] [G loss: 3.796663]\n",
      "529 [D loss: 0.082243, acc.: 93.75%] [G loss: 3.183868]\n",
      "530 [D loss: 0.057013, acc.: 100.00%] [G loss: 6.491103]\n",
      "531 [D loss: 0.173286, acc.: 90.62%] [G loss: 5.731587]\n",
      "532 [D loss: 0.135121, acc.: 96.88%] [G loss: 6.606626]\n",
      "533 [D loss: 0.805184, acc.: 59.38%] [G loss: 4.174417]\n",
      "534 [D loss: 0.098103, acc.: 96.88%] [G loss: 1.990610]\n",
      "535 [D loss: 0.176520, acc.: 93.75%] [G loss: 1.514997]\n",
      "536 [D loss: 0.631412, acc.: 71.88%] [G loss: 2.016607]\n",
      "537 [D loss: 0.040086, acc.: 100.00%] [G loss: 4.795007]\n",
      "538 [D loss: 0.024766, acc.: 100.00%] [G loss: 5.241137]\n",
      "539 [D loss: 0.070173, acc.: 96.88%] [G loss: 6.234385]\n",
      "540 [D loss: 0.332694, acc.: 87.50%] [G loss: 7.374470]\n",
      "541 [D loss: 0.031039, acc.: 100.00%] [G loss: 4.964438]\n",
      "542 [D loss: 0.062897, acc.: 96.88%] [G loss: 5.236228]\n",
      "543 [D loss: 0.075512, acc.: 96.88%] [G loss: 3.858085]\n",
      "544 [D loss: 0.263361, acc.: 93.75%] [G loss: 3.869859]\n",
      "545 [D loss: 0.092796, acc.: 96.88%] [G loss: 2.094182]\n",
      "546 [D loss: 0.107067, acc.: 93.75%] [G loss: 5.383685]\n",
      "547 [D loss: 0.009526, acc.: 100.00%] [G loss: 6.302494]\n",
      "548 [D loss: 0.270107, acc.: 90.62%] [G loss: 3.092131]\n",
      "549 [D loss: 0.262996, acc.: 93.75%] [G loss: 3.544023]\n",
      "550 [D loss: 0.024151, acc.: 100.00%] [G loss: 3.439516]\n",
      "551 [D loss: 0.286920, acc.: 87.50%] [G loss: 0.816190]\n",
      "552 [D loss: 0.189052, acc.: 90.62%] [G loss: 0.884108]\n",
      "553 [D loss: 0.174094, acc.: 96.88%] [G loss: 1.312436]\n",
      "554 [D loss: 0.034939, acc.: 100.00%] [G loss: 1.452183]\n",
      "555 [D loss: 0.168375, acc.: 96.88%] [G loss: 1.891641]\n",
      "556 [D loss: 0.048035, acc.: 100.00%] [G loss: 3.079734]\n",
      "557 [D loss: 0.181244, acc.: 90.62%] [G loss: 3.793489]\n",
      "558 [D loss: 0.018849, acc.: 100.00%] [G loss: 4.527873]\n",
      "559 [D loss: 0.005416, acc.: 100.00%] [G loss: 3.007089]\n",
      "560 [D loss: 0.187490, acc.: 90.62%] [G loss: 4.503499]\n",
      "561 [D loss: 0.120423, acc.: 93.75%] [G loss: 3.498772]\n",
      "562 [D loss: 0.014458, acc.: 100.00%] [G loss: 1.354893]\n",
      "563 [D loss: 0.053267, acc.: 96.88%] [G loss: 1.614796]\n",
      "564 [D loss: 0.510780, acc.: 75.00%] [G loss: 4.064582]\n",
      "565 [D loss: 0.027706, acc.: 100.00%] [G loss: 5.964566]\n",
      "566 [D loss: 0.038590, acc.: 100.00%] [G loss: 4.970366]\n",
      "567 [D loss: 0.068903, acc.: 96.88%] [G loss: 7.805504]\n",
      "568 [D loss: 0.129229, acc.: 100.00%] [G loss: 8.340741]\n",
      "569 [D loss: 0.743021, acc.: 75.00%] [G loss: 2.749916]\n",
      "570 [D loss: 1.098915, acc.: 56.25%] [G loss: 0.957667]\n",
      "571 [D loss: 1.399465, acc.: 59.38%] [G loss: 0.834473]\n",
      "572 [D loss: 0.615794, acc.: 75.00%] [G loss: 3.534544]\n",
      "573 [D loss: 0.066824, acc.: 96.88%] [G loss: 7.122054]\n",
      "574 [D loss: 0.475115, acc.: 81.25%] [G loss: 9.110135]\n",
      "575 [D loss: 0.394388, acc.: 81.25%] [G loss: 7.248832]\n",
      "576 [D loss: 0.654614, acc.: 71.88%] [G loss: 6.201008]\n",
      "577 [D loss: 0.079523, acc.: 96.88%] [G loss: 3.147563]\n",
      "578 [D loss: 0.556880, acc.: 84.38%] [G loss: 2.259783]\n",
      "579 [D loss: 0.192693, acc.: 93.75%] [G loss: 3.489753]\n",
      "580 [D loss: 0.130493, acc.: 96.88%] [G loss: 2.688151]\n",
      "581 [D loss: 0.167024, acc.: 93.75%] [G loss: 2.572361]\n",
      "582 [D loss: 0.131021, acc.: 96.88%] [G loss: 3.343770]\n",
      "583 [D loss: 0.166606, acc.: 100.00%] [G loss: 4.534438]\n",
      "584 [D loss: 0.084970, acc.: 93.75%] [G loss: 5.684617]\n",
      "585 [D loss: 1.591878, acc.: 31.25%] [G loss: 3.038862]\n",
      "586 [D loss: 0.075900, acc.: 96.88%] [G loss: 3.352761]\n",
      "587 [D loss: 0.371758, acc.: 84.38%] [G loss: 4.265442]\n",
      "588 [D loss: 0.292208, acc.: 93.75%] [G loss: 3.127272]\n",
      "589 [D loss: 0.007587, acc.: 100.00%] [G loss: 4.720838]\n",
      "590 [D loss: 0.349094, acc.: 84.38%] [G loss: 4.930972]\n",
      "591 [D loss: 0.202996, acc.: 93.75%] [G loss: 2.024704]\n",
      "592 [D loss: 0.210548, acc.: 93.75%] [G loss: 2.856968]\n",
      "593 [D loss: 0.054600, acc.: 96.88%] [G loss: 3.981280]\n",
      "594 [D loss: 0.226500, acc.: 87.50%] [G loss: 4.248168]\n",
      "595 [D loss: 0.047607, acc.: 100.00%] [G loss: 4.092533]\n",
      "596 [D loss: 0.276766, acc.: 87.50%] [G loss: 4.792678]\n",
      "597 [D loss: 0.482016, acc.: 75.00%] [G loss: 1.546107]\n",
      "598 [D loss: 0.381677, acc.: 75.00%] [G loss: 3.530506]\n",
      "599 [D loss: 0.163006, acc.: 87.50%] [G loss: 5.683979]\n",
      "600 [D loss: 0.869532, acc.: 53.12%] [G loss: 5.959785]\n",
      "601 [D loss: 0.376683, acc.: 93.75%] [G loss: 2.698983]\n",
      "602 [D loss: 0.223760, acc.: 96.88%] [G loss: 3.966435]\n",
      "603 [D loss: 0.159585, acc.: 93.75%] [G loss: 2.184301]\n",
      "604 [D loss: 0.093292, acc.: 96.88%] [G loss: 3.078049]\n",
      "605 [D loss: 0.044726, acc.: 96.88%] [G loss: 3.448883]\n",
      "606 [D loss: 0.073475, acc.: 100.00%] [G loss: 5.497643]\n",
      "607 [D loss: 0.189384, acc.: 90.62%] [G loss: 5.954565]\n",
      "608 [D loss: 0.030201, acc.: 100.00%] [G loss: 5.396683]\n",
      "609 [D loss: 0.098860, acc.: 100.00%] [G loss: 2.430532]\n",
      "610 [D loss: 0.027442, acc.: 100.00%] [G loss: 4.591352]\n",
      "611 [D loss: 0.259970, acc.: 90.62%] [G loss: 3.042451]\n",
      "612 [D loss: 0.778078, acc.: 68.75%] [G loss: 3.143137]\n",
      "613 [D loss: 0.389271, acc.: 81.25%] [G loss: 0.645668]\n",
      "614 [D loss: 0.799824, acc.: 59.38%] [G loss: 0.217869]\n",
      "615 [D loss: 1.344762, acc.: 53.12%] [G loss: 1.238715]\n",
      "616 [D loss: 0.141969, acc.: 93.75%] [G loss: 4.365633]\n",
      "617 [D loss: 0.173002, acc.: 93.75%] [G loss: 5.343657]\n",
      "618 [D loss: 0.213953, acc.: 90.62%] [G loss: 7.507049]\n",
      "619 [D loss: 0.599795, acc.: 78.12%] [G loss: 5.678580]\n",
      "620 [D loss: 0.528914, acc.: 78.12%] [G loss: 4.893775]\n",
      "621 [D loss: 0.406028, acc.: 84.38%] [G loss: 2.870123]\n",
      "622 [D loss: 0.081642, acc.: 100.00%] [G loss: 1.013915]\n",
      "623 [D loss: 0.909567, acc.: 68.75%] [G loss: 1.843379]\n",
      "624 [D loss: 0.726941, acc.: 75.00%] [G loss: 2.397152]\n",
      "625 [D loss: 0.559532, acc.: 71.88%] [G loss: 2.704103]\n",
      "626 [D loss: 0.307879, acc.: 84.38%] [G loss: 4.908942]\n",
      "627 [D loss: 0.151679, acc.: 93.75%] [G loss: 3.223634]\n",
      "628 [D loss: 0.261225, acc.: 84.38%] [G loss: 4.488157]\n",
      "629 [D loss: 0.732583, acc.: 62.50%] [G loss: 2.132444]\n",
      "630 [D loss: 0.079910, acc.: 96.88%] [G loss: 2.336902]\n",
      "631 [D loss: 0.122324, acc.: 93.75%] [G loss: 1.577503]\n",
      "632 [D loss: 0.370427, acc.: 87.50%] [G loss: 3.240410]\n",
      "633 [D loss: 0.243060, acc.: 81.25%] [G loss: 1.379883]\n",
      "634 [D loss: 0.263591, acc.: 87.50%] [G loss: 2.721458]\n",
      "635 [D loss: 0.224771, acc.: 90.62%] [G loss: 4.009155]\n",
      "636 [D loss: 0.200045, acc.: 93.75%] [G loss: 4.413263]\n",
      "637 [D loss: 0.322671, acc.: 90.62%] [G loss: 4.519683]\n",
      "638 [D loss: 0.158071, acc.: 96.88%] [G loss: 5.192245]\n",
      "639 [D loss: 0.121220, acc.: 96.88%] [G loss: 1.582502]\n",
      "640 [D loss: 0.459592, acc.: 78.12%] [G loss: 5.126244]\n",
      "641 [D loss: 0.186761, acc.: 93.75%] [G loss: 3.362495]\n",
      "642 [D loss: 0.091587, acc.: 100.00%] [G loss: 2.874237]\n",
      "643 [D loss: 0.204589, acc.: 93.75%] [G loss: 4.671674]\n",
      "644 [D loss: 0.060915, acc.: 96.88%] [G loss: 3.019965]\n",
      "645 [D loss: 0.207501, acc.: 93.75%] [G loss: 3.956770]\n",
      "646 [D loss: 0.138641, acc.: 93.75%] [G loss: 5.080085]\n",
      "647 [D loss: 0.075637, acc.: 96.88%] [G loss: 4.200326]\n",
      "648 [D loss: 0.240664, acc.: 90.62%] [G loss: 5.956726]\n",
      "649 [D loss: 0.594533, acc.: 71.88%] [G loss: 3.537023]\n",
      "650 [D loss: 0.581621, acc.: 71.88%] [G loss: 3.821054]\n",
      "651 [D loss: 0.549896, acc.: 75.00%] [G loss: 2.546782]\n",
      "652 [D loss: 1.428735, acc.: 46.88%] [G loss: 0.708047]\n",
      "653 [D loss: 0.143894, acc.: 90.62%] [G loss: 2.872529]\n",
      "654 [D loss: 0.268134, acc.: 84.38%] [G loss: 2.896649]\n",
      "655 [D loss: 0.039409, acc.: 100.00%] [G loss: 4.312495]\n",
      "656 [D loss: 0.101003, acc.: 100.00%] [G loss: 4.093297]\n",
      "657 [D loss: 1.469362, acc.: 50.00%] [G loss: 2.728937]\n",
      "658 [D loss: 0.656254, acc.: 65.62%] [G loss: 2.364214]\n",
      "659 [D loss: 0.630536, acc.: 68.75%] [G loss: 1.102661]\n",
      "660 [D loss: 0.424967, acc.: 81.25%] [G loss: 1.032012]\n",
      "661 [D loss: 0.351435, acc.: 75.00%] [G loss: 3.736385]\n",
      "662 [D loss: 0.157011, acc.: 93.75%] [G loss: 3.095426]\n",
      "663 [D loss: 0.159515, acc.: 96.88%] [G loss: 4.381617]\n",
      "664 [D loss: 0.042689, acc.: 100.00%] [G loss: 4.765430]\n",
      "665 [D loss: 0.070243, acc.: 96.88%] [G loss: 5.182855]\n",
      "666 [D loss: 0.388131, acc.: 71.88%] [G loss: 4.688006]\n",
      "667 [D loss: 0.493879, acc.: 75.00%] [G loss: 5.049070]\n",
      "668 [D loss: 0.156461, acc.: 96.88%] [G loss: 2.461902]\n",
      "669 [D loss: 0.469667, acc.: 71.88%] [G loss: 1.884039]\n",
      "670 [D loss: 0.540957, acc.: 68.75%] [G loss: 1.810957]\n",
      "671 [D loss: 0.232372, acc.: 93.75%] [G loss: 3.700374]\n",
      "672 [D loss: 0.055999, acc.: 100.00%] [G loss: 4.054200]\n",
      "673 [D loss: 0.128124, acc.: 100.00%] [G loss: 4.008284]\n",
      "674 [D loss: 0.400758, acc.: 81.25%] [G loss: 3.655217]\n",
      "675 [D loss: 0.186916, acc.: 96.88%] [G loss: 3.783546]\n",
      "676 [D loss: 0.489345, acc.: 75.00%] [G loss: 1.516992]\n",
      "677 [D loss: 0.401226, acc.: 75.00%] [G loss: 2.144467]\n",
      "678 [D loss: 0.119890, acc.: 96.88%] [G loss: 3.352555]\n",
      "679 [D loss: 0.079800, acc.: 93.75%] [G loss: 3.795528]\n",
      "680 [D loss: 0.450704, acc.: 75.00%] [G loss: 3.069075]\n",
      "681 [D loss: 0.235828, acc.: 90.62%] [G loss: 3.366599]\n",
      "682 [D loss: 0.068532, acc.: 100.00%] [G loss: 2.288568]\n",
      "683 [D loss: 0.181652, acc.: 96.88%] [G loss: 2.970771]\n",
      "684 [D loss: 0.030161, acc.: 100.00%] [G loss: 3.436203]\n",
      "685 [D loss: 0.104933, acc.: 96.88%] [G loss: 3.080443]\n",
      "686 [D loss: 0.284413, acc.: 87.50%] [G loss: 2.250126]\n",
      "687 [D loss: 0.308877, acc.: 84.38%] [G loss: 1.795904]\n",
      "688 [D loss: 0.103054, acc.: 93.75%] [G loss: 2.330340]\n",
      "689 [D loss: 0.475349, acc.: 78.12%] [G loss: 3.217667]\n",
      "690 [D loss: 0.087600, acc.: 100.00%] [G loss: 5.093072]\n",
      "691 [D loss: 0.041700, acc.: 100.00%] [G loss: 7.527719]\n",
      "692 [D loss: 0.475504, acc.: 81.25%] [G loss: 7.361254]\n",
      "693 [D loss: 0.348245, acc.: 87.50%] [G loss: 6.473607]\n",
      "694 [D loss: 0.293518, acc.: 84.38%] [G loss: 4.069722]\n",
      "695 [D loss: 0.286218, acc.: 84.38%] [G loss: 4.248372]\n",
      "696 [D loss: 0.021413, acc.: 100.00%] [G loss: 3.356584]\n",
      "697 [D loss: 0.358163, acc.: 75.00%] [G loss: 2.538811]\n",
      "698 [D loss: 0.144224, acc.: 93.75%] [G loss: 2.030231]\n",
      "699 [D loss: 0.410084, acc.: 84.38%] [G loss: 2.552047]\n",
      "700 [D loss: 0.028037, acc.: 100.00%] [G loss: 2.987083]\n",
      "701 [D loss: 0.071648, acc.: 100.00%] [G loss: 7.376237]\n",
      "702 [D loss: 0.233128, acc.: 87.50%] [G loss: 6.490438]\n",
      "703 [D loss: 0.654047, acc.: 75.00%] [G loss: 5.653561]\n",
      "704 [D loss: 0.057415, acc.: 100.00%] [G loss: 5.552387]\n",
      "705 [D loss: 0.360214, acc.: 84.38%] [G loss: 2.984493]\n",
      "706 [D loss: 0.132238, acc.: 96.88%] [G loss: 1.961794]\n",
      "707 [D loss: 0.188594, acc.: 93.75%] [G loss: 2.651428]\n",
      "708 [D loss: 0.075477, acc.: 100.00%] [G loss: 3.203713]\n",
      "709 [D loss: 0.057166, acc.: 100.00%] [G loss: 2.785729]\n",
      "710 [D loss: 0.057635, acc.: 100.00%] [G loss: 5.022011]\n",
      "711 [D loss: 0.032842, acc.: 100.00%] [G loss: 5.949313]\n",
      "712 [D loss: 0.069260, acc.: 100.00%] [G loss: 4.876738]\n",
      "713 [D loss: 0.077608, acc.: 100.00%] [G loss: 4.691579]\n",
      "714 [D loss: 0.216786, acc.: 84.38%] [G loss: 6.799805]\n",
      "715 [D loss: 0.042706, acc.: 100.00%] [G loss: 3.230148]\n",
      "716 [D loss: 0.391122, acc.: 78.12%] [G loss: 5.904100]\n",
      "717 [D loss: 0.256492, acc.: 87.50%] [G loss: 3.562122]\n",
      "718 [D loss: 0.067866, acc.: 96.88%] [G loss: 3.005332]\n",
      "719 [D loss: 0.057134, acc.: 100.00%] [G loss: 4.946397]\n",
      "720 [D loss: 0.017062, acc.: 100.00%] [G loss: 6.014603]\n",
      "721 [D loss: 0.422946, acc.: 78.12%] [G loss: 6.500498]\n",
      "722 [D loss: 0.250922, acc.: 93.75%] [G loss: 2.602876]\n",
      "723 [D loss: 0.156388, acc.: 93.75%] [G loss: 3.893232]\n",
      "724 [D loss: 0.068653, acc.: 96.88%] [G loss: 4.224782]\n",
      "725 [D loss: 0.883748, acc.: 56.25%] [G loss: 1.879757]\n",
      "726 [D loss: 0.088460, acc.: 96.88%] [G loss: 2.138968]\n",
      "727 [D loss: 0.333470, acc.: 84.38%] [G loss: 4.138628]\n",
      "728 [D loss: 0.121418, acc.: 96.88%] [G loss: 3.706327]\n",
      "729 [D loss: 0.090244, acc.: 93.75%] [G loss: 3.603480]\n",
      "730 [D loss: 0.043410, acc.: 100.00%] [G loss: 6.659221]\n",
      "731 [D loss: 0.601872, acc.: 71.88%] [G loss: 4.111461]\n",
      "732 [D loss: 0.453804, acc.: 84.38%] [G loss: 1.918736]\n",
      "733 [D loss: 0.479359, acc.: 84.38%] [G loss: 3.315521]\n",
      "734 [D loss: 0.294969, acc.: 87.50%] [G loss: 3.041406]\n",
      "735 [D loss: 0.487111, acc.: 78.12%] [G loss: 2.711163]\n",
      "736 [D loss: 0.105128, acc.: 96.88%] [G loss: 3.756968]\n",
      "737 [D loss: 0.308552, acc.: 87.50%] [G loss: 6.216682]\n",
      "738 [D loss: 0.015532, acc.: 100.00%] [G loss: 3.277007]\n",
      "739 [D loss: 0.196420, acc.: 93.75%] [G loss: 8.006317]\n",
      "740 [D loss: 0.140757, acc.: 93.75%] [G loss: 5.488614]\n",
      "741 [D loss: 0.004726, acc.: 100.00%] [G loss: 5.766023]\n",
      "742 [D loss: 0.077167, acc.: 96.88%] [G loss: 3.899825]\n",
      "743 [D loss: 0.768437, acc.: 71.88%] [G loss: 5.769904]\n",
      "744 [D loss: 0.125123, acc.: 96.88%] [G loss: 2.699070]\n",
      "745 [D loss: 0.329075, acc.: 93.75%] [G loss: 8.173297]\n",
      "746 [D loss: 0.286303, acc.: 84.38%] [G loss: 3.927061]\n",
      "747 [D loss: 0.207171, acc.: 93.75%] [G loss: 1.880422]\n",
      "748 [D loss: 0.247549, acc.: 90.62%] [G loss: 1.076550]\n",
      "749 [D loss: 0.282617, acc.: 93.75%] [G loss: 2.670311]\n",
      "750 [D loss: 0.129380, acc.: 90.62%] [G loss: 2.338835]\n",
      "751 [D loss: 0.376178, acc.: 84.38%] [G loss: 2.865106]\n",
      "752 [D loss: 0.022409, acc.: 100.00%] [G loss: 4.100167]\n",
      "753 [D loss: 2.042851, acc.: 50.00%] [G loss: 4.699323]\n",
      "754 [D loss: 0.597778, acc.: 65.62%] [G loss: 1.239804]\n",
      "755 [D loss: 0.354437, acc.: 84.38%] [G loss: 1.599929]\n",
      "756 [D loss: 0.332009, acc.: 87.50%] [G loss: 2.358226]\n",
      "757 [D loss: 0.116212, acc.: 96.88%] [G loss: 3.630463]\n",
      "758 [D loss: 0.067589, acc.: 100.00%] [G loss: 5.157395]\n",
      "759 [D loss: 0.051668, acc.: 100.00%] [G loss: 7.094706]\n",
      "760 [D loss: 0.065418, acc.: 96.88%] [G loss: 6.297334]\n",
      "761 [D loss: 0.864771, acc.: 59.38%] [G loss: 6.615051]\n",
      "762 [D loss: 0.048857, acc.: 100.00%] [G loss: 4.340762]\n",
      "763 [D loss: 0.413557, acc.: 78.12%] [G loss: 2.338678]\n",
      "764 [D loss: 0.757113, acc.: 56.25%] [G loss: 4.056705]\n",
      "765 [D loss: 0.034927, acc.: 100.00%] [G loss: 1.891063]\n",
      "766 [D loss: 0.200414, acc.: 90.62%] [G loss: 6.359354]\n",
      "767 [D loss: 0.042513, acc.: 96.88%] [G loss: 6.651774]\n",
      "768 [D loss: 0.068844, acc.: 96.88%] [G loss: 4.474942]\n",
      "769 [D loss: 0.031165, acc.: 100.00%] [G loss: 5.800023]\n",
      "770 [D loss: 0.354956, acc.: 84.38%] [G loss: 3.998685]\n",
      "771 [D loss: 0.025118, acc.: 100.00%] [G loss: 5.475205]\n",
      "772 [D loss: 0.055572, acc.: 100.00%] [G loss: 3.808868]\n",
      "773 [D loss: 0.136278, acc.: 96.88%] [G loss: 7.788048]\n",
      "774 [D loss: 0.148955, acc.: 93.75%] [G loss: 5.863987]\n",
      "775 [D loss: 0.119195, acc.: 93.75%] [G loss: 4.735663]\n",
      "776 [D loss: 0.047576, acc.: 96.88%] [G loss: 3.154794]\n",
      "777 [D loss: 0.301182, acc.: 84.38%] [G loss: 6.791071]\n",
      "778 [D loss: 0.069893, acc.: 96.88%] [G loss: 4.807338]\n",
      "779 [D loss: 0.098416, acc.: 96.88%] [G loss: 6.531705]\n",
      "780 [D loss: 0.009295, acc.: 100.00%] [G loss: 4.687861]\n",
      "781 [D loss: 0.353868, acc.: 84.38%] [G loss: 3.675613]\n",
      "782 [D loss: 0.024894, acc.: 100.00%] [G loss: 2.146788]\n",
      "783 [D loss: 0.216009, acc.: 87.50%] [G loss: 0.936356]\n",
      "784 [D loss: 0.411048, acc.: 78.12%] [G loss: 2.852394]\n",
      "785 [D loss: 0.412005, acc.: 84.38%] [G loss: 1.819862]\n",
      "786 [D loss: 0.507172, acc.: 65.62%] [G loss: 5.941381]\n",
      "787 [D loss: 0.128535, acc.: 96.88%] [G loss: 6.036852]\n",
      "788 [D loss: 0.160074, acc.: 87.50%] [G loss: 6.178820]\n",
      "789 [D loss: 1.050525, acc.: 62.50%] [G loss: 6.272697]\n",
      "790 [D loss: 0.211022, acc.: 87.50%] [G loss: 1.390017]\n",
      "791 [D loss: 0.137398, acc.: 96.88%] [G loss: 1.907261]\n",
      "792 [D loss: 0.486018, acc.: 78.12%] [G loss: 1.563848]\n",
      "793 [D loss: 0.776450, acc.: 65.62%] [G loss: 1.429296]\n",
      "794 [D loss: 0.473478, acc.: 75.00%] [G loss: 2.498031]\n",
      "795 [D loss: 0.084402, acc.: 96.88%] [G loss: 2.360352]\n",
      "796 [D loss: 0.076095, acc.: 96.88%] [G loss: 7.777112]\n",
      "797 [D loss: 0.246225, acc.: 90.62%] [G loss: 7.531699]\n",
      "798 [D loss: 0.196444, acc.: 90.62%] [G loss: 8.915640]\n",
      "799 [D loss: 0.399075, acc.: 90.62%] [G loss: 7.642475]\n",
      "800 [D loss: 0.135875, acc.: 93.75%] [G loss: 5.887116]\n",
      "801 [D loss: 0.180806, acc.: 93.75%] [G loss: 3.003782]\n",
      "802 [D loss: 0.312643, acc.: 87.50%] [G loss: 1.695878]\n",
      "803 [D loss: 0.205298, acc.: 90.62%] [G loss: 2.800292]\n",
      "804 [D loss: 0.860755, acc.: 65.62%] [G loss: 1.958375]\n",
      "805 [D loss: 0.187436, acc.: 93.75%] [G loss: 2.134549]\n",
      "806 [D loss: 0.124079, acc.: 96.88%] [G loss: 4.956575]\n",
      "807 [D loss: 0.580510, acc.: 68.75%] [G loss: 2.772248]\n",
      "808 [D loss: 0.095750, acc.: 96.88%] [G loss: 1.680159]\n",
      "809 [D loss: 0.514430, acc.: 68.75%] [G loss: 2.578448]\n",
      "810 [D loss: 0.010030, acc.: 100.00%] [G loss: 2.524915]\n",
      "811 [D loss: 0.172360, acc.: 93.75%] [G loss: 4.052196]\n",
      "812 [D loss: 0.253422, acc.: 90.62%] [G loss: 3.936411]\n",
      "813 [D loss: 0.280909, acc.: 90.62%] [G loss: 3.966536]\n",
      "814 [D loss: 0.018607, acc.: 100.00%] [G loss: 4.257383]\n",
      "815 [D loss: 0.689151, acc.: 75.00%] [G loss: 4.503070]\n",
      "816 [D loss: 0.978868, acc.: 59.38%] [G loss: 4.353715]\n",
      "817 [D loss: 0.968888, acc.: 56.25%] [G loss: 4.075418]\n",
      "818 [D loss: 0.248913, acc.: 87.50%] [G loss: 2.959231]\n",
      "819 [D loss: 0.222357, acc.: 90.62%] [G loss: 4.018751]\n",
      "820 [D loss: 0.100940, acc.: 100.00%] [G loss: 4.478537]\n",
      "821 [D loss: 0.489573, acc.: 75.00%] [G loss: 4.904612]\n",
      "822 [D loss: 0.540969, acc.: 81.25%] [G loss: 1.910726]\n",
      "823 [D loss: 0.742192, acc.: 65.62%] [G loss: 1.007765]\n",
      "824 [D loss: 0.547453, acc.: 71.88%] [G loss: 4.541205]\n",
      "825 [D loss: 0.126823, acc.: 93.75%] [G loss: 4.054060]\n",
      "826 [D loss: 0.280155, acc.: 93.75%] [G loss: 5.933123]\n",
      "827 [D loss: 0.560539, acc.: 75.00%] [G loss: 6.420575]\n",
      "828 [D loss: 0.091973, acc.: 100.00%] [G loss: 5.346130]\n",
      "829 [D loss: 0.303535, acc.: 90.62%] [G loss: 3.924875]\n",
      "830 [D loss: 0.186136, acc.: 93.75%] [G loss: 3.153226]\n",
      "831 [D loss: 0.144527, acc.: 96.88%] [G loss: 2.937177]\n",
      "832 [D loss: 0.234498, acc.: 90.62%] [G loss: 4.574383]\n",
      "833 [D loss: 0.108939, acc.: 96.88%] [G loss: 4.973332]\n",
      "834 [D loss: 0.067210, acc.: 100.00%] [G loss: 4.677188]\n",
      "835 [D loss: 0.120880, acc.: 93.75%] [G loss: 5.413898]\n",
      "836 [D loss: 0.112965, acc.: 96.88%] [G loss: 3.764972]\n",
      "837 [D loss: 0.101952, acc.: 96.88%] [G loss: 5.463524]\n",
      "838 [D loss: 0.134334, acc.: 96.88%] [G loss: 3.034414]\n",
      "839 [D loss: 0.049087, acc.: 100.00%] [G loss: 4.384759]\n",
      "840 [D loss: 0.190498, acc.: 90.62%] [G loss: 5.233902]\n",
      "841 [D loss: 0.051291, acc.: 96.88%] [G loss: 3.763305]\n",
      "842 [D loss: 0.056967, acc.: 96.88%] [G loss: 6.247797]\n",
      "843 [D loss: 0.098940, acc.: 100.00%] [G loss: 7.062718]\n",
      "844 [D loss: 0.024544, acc.: 100.00%] [G loss: 6.097444]\n",
      "845 [D loss: 0.035568, acc.: 100.00%] [G loss: 5.528725]\n",
      "846 [D loss: 0.408840, acc.: 78.12%] [G loss: 3.332596]\n",
      "847 [D loss: 0.009074, acc.: 100.00%] [G loss: 5.362903]\n",
      "848 [D loss: 0.046197, acc.: 100.00%] [G loss: 3.946967]\n",
      "849 [D loss: 0.292109, acc.: 81.25%] [G loss: 3.109550]\n",
      "850 [D loss: 0.396384, acc.: 81.25%] [G loss: 2.550084]\n",
      "851 [D loss: 0.068974, acc.: 100.00%] [G loss: 4.011273]\n",
      "852 [D loss: 0.156301, acc.: 93.75%] [G loss: 6.320226]\n",
      "853 [D loss: 1.897548, acc.: 46.88%] [G loss: 2.833397]\n",
      "854 [D loss: 0.144468, acc.: 96.88%] [G loss: 3.393061]\n",
      "855 [D loss: 0.501212, acc.: 78.12%] [G loss: 2.349907]\n",
      "856 [D loss: 0.389619, acc.: 93.75%] [G loss: 4.449432]\n",
      "857 [D loss: 0.080895, acc.: 100.00%] [G loss: 5.041902]\n",
      "858 [D loss: 0.140115, acc.: 93.75%] [G loss: 5.569222]\n",
      "859 [D loss: 0.364617, acc.: 90.62%] [G loss: 5.241661]\n",
      "860 [D loss: 0.211553, acc.: 87.50%] [G loss: 6.831115]\n",
      "861 [D loss: 0.595493, acc.: 68.75%] [G loss: 2.780060]\n",
      "862 [D loss: 0.080398, acc.: 96.88%] [G loss: 2.638456]\n",
      "863 [D loss: 0.105431, acc.: 96.88%] [G loss: 1.439823]\n",
      "864 [D loss: 0.330704, acc.: 87.50%] [G loss: 2.381082]\n",
      "865 [D loss: 0.122775, acc.: 96.88%] [G loss: 2.660648]\n",
      "866 [D loss: 0.079306, acc.: 100.00%] [G loss: 4.817542]\n",
      "867 [D loss: 0.094368, acc.: 93.75%] [G loss: 2.907259]\n",
      "868 [D loss: 0.166581, acc.: 96.88%] [G loss: 3.660700]\n",
      "869 [D loss: 0.420623, acc.: 90.62%] [G loss: 5.990005]\n",
      "870 [D loss: 0.072653, acc.: 96.88%] [G loss: 8.472563]\n",
      "871 [D loss: 0.246647, acc.: 84.38%] [G loss: 5.785642]\n",
      "872 [D loss: 0.258290, acc.: 84.38%] [G loss: 3.333258]\n",
      "873 [D loss: 0.278210, acc.: 84.38%] [G loss: 1.833936]\n",
      "874 [D loss: 0.432731, acc.: 81.25%] [G loss: 2.095305]\n",
      "875 [D loss: 0.154265, acc.: 93.75%] [G loss: 1.200149]\n",
      "876 [D loss: 0.739544, acc.: 56.25%] [G loss: 2.895653]\n",
      "877 [D loss: 0.023359, acc.: 100.00%] [G loss: 8.938637]\n",
      "878 [D loss: 0.123549, acc.: 93.75%] [G loss: 7.703601]\n",
      "879 [D loss: 1.119202, acc.: 68.75%] [G loss: 10.592266]\n",
      "880 [D loss: 0.139818, acc.: 90.62%] [G loss: 5.003215]\n",
      "881 [D loss: 0.535156, acc.: 78.12%] [G loss: 3.419730]\n",
      "882 [D loss: 0.029449, acc.: 100.00%] [G loss: 1.918259]\n",
      "883 [D loss: 1.134024, acc.: 56.25%] [G loss: 1.833638]\n",
      "884 [D loss: 0.075918, acc.: 100.00%] [G loss: 1.070217]\n",
      "885 [D loss: 0.260865, acc.: 87.50%] [G loss: 2.553877]\n",
      "886 [D loss: 0.238773, acc.: 87.50%] [G loss: 2.569685]\n",
      "887 [D loss: 0.344833, acc.: 78.12%] [G loss: 3.837731]\n",
      "888 [D loss: 0.108590, acc.: 96.88%] [G loss: 2.565832]\n",
      "889 [D loss: 0.407748, acc.: 78.12%] [G loss: 4.504586]\n",
      "890 [D loss: 0.245086, acc.: 87.50%] [G loss: 5.133792]\n",
      "891 [D loss: 0.251504, acc.: 90.62%] [G loss: 5.744730]\n",
      "892 [D loss: 0.134240, acc.: 96.88%] [G loss: 3.973991]\n",
      "893 [D loss: 0.064188, acc.: 100.00%] [G loss: 5.467307]\n",
      "894 [D loss: 0.497281, acc.: 71.88%] [G loss: 4.972180]\n",
      "895 [D loss: 0.154400, acc.: 96.88%] [G loss: 3.252607]\n",
      "896 [D loss: 1.081570, acc.: 62.50%] [G loss: 2.738336]\n",
      "897 [D loss: 0.176244, acc.: 96.88%] [G loss: 1.019785]\n",
      "898 [D loss: 0.398908, acc.: 84.38%] [G loss: 1.796674]\n",
      "899 [D loss: 0.086551, acc.: 96.88%] [G loss: 3.207065]\n",
      "900 [D loss: 0.072787, acc.: 100.00%] [G loss: 2.766313]\n",
      "901 [D loss: 0.047681, acc.: 100.00%] [G loss: 3.102704]\n",
      "902 [D loss: 0.095384, acc.: 96.88%] [G loss: 6.693596]\n",
      "903 [D loss: 0.193674, acc.: 87.50%] [G loss: 3.421597]\n",
      "904 [D loss: 0.130944, acc.: 93.75%] [G loss: 4.160416]\n",
      "905 [D loss: 0.271432, acc.: 87.50%] [G loss: 1.894034]\n",
      "906 [D loss: 0.046635, acc.: 100.00%] [G loss: 2.370233]\n",
      "907 [D loss: 0.403911, acc.: 75.00%] [G loss: 1.615600]\n",
      "908 [D loss: 0.336250, acc.: 84.38%] [G loss: 3.514737]\n",
      "909 [D loss: 0.247468, acc.: 87.50%] [G loss: 3.543747]\n",
      "910 [D loss: 0.132335, acc.: 96.88%] [G loss: 1.590559]\n",
      "911 [D loss: 0.132024, acc.: 93.75%] [G loss: 3.542815]\n",
      "912 [D loss: 0.223076, acc.: 87.50%] [G loss: 3.528415]\n",
      "913 [D loss: 0.079991, acc.: 96.88%] [G loss: 3.024753]\n",
      "914 [D loss: 0.141310, acc.: 96.88%] [G loss: 4.366493]\n",
      "915 [D loss: 0.378222, acc.: 81.25%] [G loss: 4.522433]\n",
      "916 [D loss: 0.242494, acc.: 87.50%] [G loss: 3.970921]\n",
      "917 [D loss: 0.167026, acc.: 93.75%] [G loss: 1.319993]\n",
      "918 [D loss: 0.363347, acc.: 84.38%] [G loss: 1.458554]\n",
      "919 [D loss: 0.079801, acc.: 100.00%] [G loss: 2.492716]\n",
      "920 [D loss: 0.567781, acc.: 71.88%] [G loss: 3.578376]\n",
      "921 [D loss: 0.149355, acc.: 93.75%] [G loss: 7.900390]\n",
      "922 [D loss: 0.586479, acc.: 65.62%] [G loss: 5.765453]\n",
      "923 [D loss: 1.423373, acc.: 46.88%] [G loss: 3.957955]\n",
      "924 [D loss: 0.540667, acc.: 81.25%] [G loss: 3.809064]\n",
      "925 [D loss: 0.160202, acc.: 93.75%] [G loss: 4.558279]\n",
      "926 [D loss: 0.286928, acc.: 84.38%] [G loss: 3.397261]\n",
      "927 [D loss: 0.319021, acc.: 84.38%] [G loss: 2.646828]\n",
      "928 [D loss: 0.908402, acc.: 50.00%] [G loss: 1.373566]\n",
      "929 [D loss: 0.607063, acc.: 75.00%] [G loss: 2.183682]\n",
      "930 [D loss: 0.558026, acc.: 68.75%] [G loss: 3.885553]\n",
      "931 [D loss: 0.116967, acc.: 93.75%] [G loss: 5.227388]\n",
      "932 [D loss: 0.204549, acc.: 90.62%] [G loss: 6.585937]\n",
      "933 [D loss: 0.153753, acc.: 93.75%] [G loss: 7.688830]\n",
      "934 [D loss: 0.372932, acc.: 81.25%] [G loss: 6.908568]\n",
      "935 [D loss: 0.822601, acc.: 62.50%] [G loss: 6.224130]\n",
      "936 [D loss: 0.444082, acc.: 75.00%] [G loss: 2.261404]\n",
      "937 [D loss: 0.123860, acc.: 90.62%] [G loss: 0.491575]\n",
      "938 [D loss: 0.812240, acc.: 65.62%] [G loss: 0.388832]\n",
      "939 [D loss: 0.332459, acc.: 81.25%] [G loss: 1.466219]\n",
      "940 [D loss: 0.262876, acc.: 87.50%] [G loss: 2.736035]\n",
      "941 [D loss: 0.268227, acc.: 90.62%] [G loss: 3.723509]\n",
      "942 [D loss: 0.618760, acc.: 75.00%] [G loss: 5.081368]\n",
      "943 [D loss: 0.427508, acc.: 75.00%] [G loss: 5.099659]\n",
      "944 [D loss: 0.180081, acc.: 90.62%] [G loss: 5.399486]\n",
      "945 [D loss: 0.029823, acc.: 100.00%] [G loss: 5.434270]\n",
      "946 [D loss: 0.438439, acc.: 78.12%] [G loss: 4.944635]\n",
      "947 [D loss: 0.410163, acc.: 84.38%] [G loss: 4.320487]\n",
      "948 [D loss: 0.275691, acc.: 90.62%] [G loss: 4.779073]\n",
      "949 [D loss: 0.092100, acc.: 100.00%] [G loss: 3.894876]\n",
      "950 [D loss: 0.198494, acc.: 90.62%] [G loss: 3.016897]\n",
      "951 [D loss: 0.135861, acc.: 93.75%] [G loss: 2.428885]\n",
      "952 [D loss: 0.160439, acc.: 90.62%] [G loss: 5.058143]\n",
      "953 [D loss: 0.080883, acc.: 100.00%] [G loss: 2.840478]\n",
      "954 [D loss: 0.016949, acc.: 100.00%] [G loss: 3.279337]\n",
      "955 [D loss: 0.311272, acc.: 87.50%] [G loss: 1.823218]\n",
      "956 [D loss: 0.066705, acc.: 100.00%] [G loss: 3.766406]\n",
      "957 [D loss: 0.402472, acc.: 84.38%] [G loss: 4.008730]\n",
      "958 [D loss: 0.192003, acc.: 93.75%] [G loss: 3.878024]\n",
      "959 [D loss: 0.034882, acc.: 100.00%] [G loss: 3.598401]\n",
      "960 [D loss: 0.317696, acc.: 87.50%] [G loss: 3.750017]\n",
      "961 [D loss: 0.045830, acc.: 100.00%] [G loss: 3.295528]\n",
      "962 [D loss: 0.244153, acc.: 81.25%] [G loss: 1.139707]\n",
      "963 [D loss: 0.281545, acc.: 90.62%] [G loss: 5.164178]\n",
      "964 [D loss: 0.228384, acc.: 90.62%] [G loss: 5.349406]\n",
      "965 [D loss: 0.155479, acc.: 96.88%] [G loss: 5.298424]\n",
      "966 [D loss: 0.028756, acc.: 100.00%] [G loss: 6.977373]\n",
      "967 [D loss: 0.484050, acc.: 75.00%] [G loss: 3.803560]\n",
      "968 [D loss: 0.083059, acc.: 96.88%] [G loss: 4.513999]\n",
      "969 [D loss: 0.060514, acc.: 100.00%] [G loss: 2.722811]\n",
      "970 [D loss: 0.015855, acc.: 100.00%] [G loss: 2.375073]\n",
      "971 [D loss: 0.487653, acc.: 71.88%] [G loss: 1.825183]\n",
      "972 [D loss: 0.008083, acc.: 100.00%] [G loss: 2.868269]\n",
      "973 [D loss: 0.124075, acc.: 100.00%] [G loss: 4.346241]\n",
      "974 [D loss: 0.037389, acc.: 100.00%] [G loss: 5.092019]\n",
      "975 [D loss: 0.647462, acc.: 68.75%] [G loss: 2.674474]\n",
      "976 [D loss: 0.358537, acc.: 84.38%] [G loss: 3.992514]\n",
      "977 [D loss: 0.124744, acc.: 96.88%] [G loss: 2.443014]\n",
      "978 [D loss: 0.863780, acc.: 53.12%] [G loss: 2.850814]\n",
      "979 [D loss: 0.089729, acc.: 100.00%] [G loss: 2.841005]\n",
      "980 [D loss: 0.191558, acc.: 90.62%] [G loss: 1.824008]\n",
      "981 [D loss: 0.945768, acc.: 59.38%] [G loss: 2.374067]\n",
      "982 [D loss: 0.253246, acc.: 84.38%] [G loss: 2.741951]\n",
      "983 [D loss: 0.132331, acc.: 93.75%] [G loss: 4.066625]\n",
      "984 [D loss: 0.606677, acc.: 71.88%] [G loss: 3.340131]\n",
      "985 [D loss: 0.063108, acc.: 100.00%] [G loss: 5.221595]\n",
      "986 [D loss: 0.083897, acc.: 100.00%] [G loss: 4.055702]\n",
      "987 [D loss: 0.606214, acc.: 62.50%] [G loss: 3.432759]\n",
      "988 [D loss: 0.388799, acc.: 78.12%] [G loss: 3.607297]\n",
      "989 [D loss: 0.130906, acc.: 96.88%] [G loss: 4.575969]\n",
      "990 [D loss: 0.056770, acc.: 100.00%] [G loss: 4.837982]\n",
      "991 [D loss: 0.156702, acc.: 93.75%] [G loss: 1.707860]\n",
      "992 [D loss: 0.260417, acc.: 87.50%] [G loss: 3.513040]\n",
      "993 [D loss: 0.063505, acc.: 100.00%] [G loss: 3.943520]\n",
      "994 [D loss: 0.226232, acc.: 90.62%] [G loss: 1.632353]\n",
      "995 [D loss: 0.039438, acc.: 100.00%] [G loss: 4.144408]\n",
      "996 [D loss: 0.129978, acc.: 96.88%] [G loss: 2.900237]\n",
      "997 [D loss: 0.885094, acc.: 59.38%] [G loss: 3.902273]\n",
      "998 [D loss: 0.143449, acc.: 93.75%] [G loss: 5.394151]\n",
      "999 [D loss: 0.399899, acc.: 81.25%] [G loss: 7.782410]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 1000\n",
    "half_batch = batch_size//2\n",
    "\n",
    "for ep in range(epochs):\n",
    "    x = train_data.get_batch(half_batch)[1]\n",
    "    y = train_data.get_batch(half_batch)[0]\n",
    "    y = generator.predict(y)\n",
    "    \n",
    "    d_loss_real = discriminator.train_on_batch(x, np.ones((half_batch, 1)))\n",
    "    d_loss_fake = discriminator.train_on_batch(y, np.zeros((half_batch, 1)))\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    \n",
    "    x = train_data.get_batch(batch_size)[0]\n",
    "    g_loss = combined.train_on_batch(x, np.ones((batch_size, 1)))\n",
    "    print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (ep, d_loss[0], 100*d_loss[1], g_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(152, 152, 1)\n"
     ]
    }
   ],
   "source": [
    "import scipy.ndimage.filters as fi\n",
    "import math\n",
    "import numpy as np\n",
    "from uniio import *\n",
    "\n",
    "def filter2D(kernlen, s, fac):\n",
    "    dirac = np.zeros((kernlen, kernlen))\n",
    "    dirac[kernlen//2, kernlen//2] = 1\n",
    "    return np.clip(fi.gaussian_filter(dirac, s) * fac, a_min=None, a_max=1.0)\n",
    "\n",
    "result = np.ndarray(shape=(150,150,1), dtype=float)\n",
    "#weights = np.ndarray(shape=(data_param.res,data_param.res,1), dtype=float)\n",
    "#weights.fill(0)\n",
    "\n",
    "ps = 5//2\n",
    "hps = 15//2\n",
    "\n",
    "border = int(math.ceil(hps-(ps*3)))\n",
    "\n",
    "result=np.pad(result,((border,border),(border,border),(0,0)),mode=\"edge\")\n",
    "#weights=np.pad(weights,((border,border),(border,border),(0,0)),mode=\"edge\")\n",
    "print(result.shape)\n",
    "dataset = 18#nn_param.train_data_count\n",
    "\n",
    "input_path = \"2D_data/lowres/sph_2D_v02-01_d018_%03d\"\n",
    "ref_input_path = \"2D_data/highres/ref_sph_2D_v02_d018_%03d_sdf.uni\"\n",
    "output_path = \"2D_data/result/sph_2D_v04_d018_%03d_result.uni\"\n",
    "\n",
    "elem_min = np.vectorize(lambda x,y: min(x,y))\n",
    "circular_filter = filter2D(15, 15*0.2, 500)\n",
    "\n",
    "for t in range(5, 15):\n",
    "    result.fill(1)\n",
    "    hdr, source = readUni(input_path%t+\"_sdf.uni\")\n",
    "    \n",
    "    for x in range (ps, 50-ps, 1):\n",
    "        for y in range(ps, 50-ps, 1):\n",
    "            if(abs(source[0,x,y,0]) < 0.5):\n",
    "                x0=x-ps\n",
    "                x1=x+ps+1\n",
    "                y0=y-ps\n",
    "                y1=y+ps+1\n",
    "                \n",
    "                data = np.array([source[0,x0:x1,y0:y1]]) * 12.\n",
    "                data[0,:,:,0] = np.tanh(data[0,:,:,0])\n",
    "                \n",
    "                predict = generator.predict(x=data, batch_size=1)\n",
    "                predict = np.arctanh(np.clip(predict,-.999999,.999999))\n",
    "                    \n",
    "                predict = predict * circular_filter/4.\n",
    "\n",
    "                x0=int(3*x)-hps+border\n",
    "                x1=int(3*x)+hps+border+1\n",
    "                y0=int(3*y)-hps+border\n",
    "                y1=int(3*y)+hps+border+1\n",
    "\n",
    "                result[x0:x1,y0:y1,0] = elem_min(result[x0:x1,y0:y1,0], predict[0,:,:,0])\n",
    "\n",
    "    hdr['dimX'] = 150\n",
    "    hdr['dimY'] = 150\n",
    "\n",
    "    #print(result[border:data_param.res+border,border:data_param.res+border,0].shape)\n",
    "    writeUni(output_path%t, hdr, result[border:150+border,border:150+border,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scr_path = \"gan_v3_d%03d\" % dataset + \"_%03d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "verbose = False\n",
    "\n",
    "manta_loc = '2D_SPH/'\n",
    "def run_manta(scene, param={},run=True,logfile=\"\"):\n",
    "    command = [manta_loc+\"build/manta\", manta_loc+scene]\n",
    "\n",
    "    for k, v in param.items():\n",
    "        command += [k, str(v)]\n",
    "        \n",
    "    if logfile != \"\":\n",
    "        command += [\">\",logfile]\n",
    "        command += [\"; echo\", \"output written into %s\" % logfile]\n",
    "        \n",
    "    print(\" \".join(command) + \"\\n\")\n",
    "    \n",
    "    if not run: return\n",
    "\n",
    "    proc = Popen(command, stdin=None, stdout=PIPE, stderr=PIPE)\n",
    "\n",
    "    if verbose:\n",
    "        for line in proc.stdout:\n",
    "            print(line.decode('utf-8'))\n",
    "    for line in proc.stderr:\n",
    "        print(line.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D_SPH/build/manta 2D_SPH/scenes/show_particles.py sres 4 sdf 2D_data/result/sph_2D_v04_d018_%03d_result.uni t_end 15 res 150 t_start 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param = {}\n",
    "dataset = 0\n",
    "\n",
    "# show result\n",
    "param['sdf'] = output_path\n",
    "param['t_start'] = 5\n",
    "param['t_end'] = 15\n",
    "param['res'] = 150\n",
    "param['sres'] = 4\n",
    "\n",
    "#param['scr'] = '2D_data/screenshots/test/' + scr_path + \"_res.png\"\n",
    "\n",
    "run_manta(\"scenes/show_particles.py\", param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
