{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Particles 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Dataset\n",
    "Pipeline:\n",
    "* Generation of high-res data (reference) using some random cubes of water\n",
    "* Extracting relevant data (particle data and grid data) like e.g. sdf, velocity, pressure, density...\n",
    "* Down-sampling of particles (by a given factor) and generation of the low-res data (source)\n",
    "* Extract corresponding and relevant patches on the surface from the data-set pairs (considering the low-res data)\n",
    "* Use patche-pairs to train the NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key                  value                     description\n",
      "\n",
      "train                train/train_02.txt        train config location\n",
      "id                   v03                       id of the config version\n",
      "preprocess           preprocess/pre_02.txt     pre-processing config location\n",
      "data                 data/data_00.txt          data config location\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"manta/scenes/tools\")\n",
    "sys.path.append(\"hungarian/\")\n",
    "import json\n",
    "import os\n",
    "\n",
    "def print_config(config, desc):\n",
    "    print(\"{:<20} {:<25} {:<10}\\n\".format(\"key\", \"value\", \"description\"))\n",
    "    for k, v in config.items():\n",
    "        print(\"{:<20} {:<25} {:<10}\".format(k, str(v), desc[k]))\n",
    "\n",
    "config_path = \"config/version_03.txt\"\n",
    "data_path = \"2d_data/\"\n",
    "dataset = 19\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.loads(f.read())\n",
    "\n",
    "print_config(config, {\n",
    "        \"preprocess\" : \"pre-processing config location\",\n",
    "        \"id\" : \"id of the config version\",\n",
    "        \"data\" : \"data config location\",\n",
    "        \"train\" : \"train config location\"\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key                  value                     description\n",
      "\n",
      "max_cnt              5                         maximum count of boxes\n",
      "data_count           20                        count of generated datasets\n",
      "res                  150                       resolution of high-res grid\n",
      "max_scale            0.3                       maximum scale of boxes for data-set generation\n",
      "var2                 0                         factor of two drop shooting against other\n",
      "prefix               sph_2D                    prefix of the generated data\n",
      "seed                 123412144                 seed for random data-set generation\n",
      "fps                  30                        frames per second, the velocity of the simulation\n",
      "max_pos              0.8                       maximum x-position of boxes\n",
      "max_h                0.1                       maximum start-height of boxes\n",
      "min_scale            0.05                      minimum scale of boxes for data-set generation\n",
      "frame_count          20                        count of generated frames\n",
      "circ_vel             100.0                     velocity used for colliding drops\n",
      "sub_res              2                         count of particles per cell (per dimension)\n",
      "min_pos              0.2                       minimum x-position of boxes\n",
      "var1                 0.5                       factor of drop falling in basin data-sets (var0 is 1 - var1 - var2)\n",
      "id                   dat00                     id of the config version\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.dirname(config_path) + '/' + config['data'], 'r') as f:\n",
    "    data_config = json.loads(f.read())\n",
    "print_config(data_config, {\n",
    "        \"id\":             \"id of the config version\",\n",
    "        \"prefix\":         \"prefix of the generated data\",\n",
    "        \"fps\":            \"frames per second, the velocity of the simulation\",\n",
    "        \"frame_count\":    \"count of generated frames\",\n",
    "        \"data_count\":     \"count of generated datasets\",\n",
    "        \"sub_res\":        \"count of particles per cell (per dimension)\",\n",
    "        \"res\":            \"resolution of high-res grid\",\n",
    "        \"var1\":           \"factor of drop falling in basin data-sets (var0 is 1 - var1 - var2)\",\n",
    "        \"var2\":           \"factor of two drop shooting against other\",\n",
    "        \"seed\":           \"seed for random data-set generation\",\n",
    "        \"min_scale\":      \"minimum scale of boxes for data-set generation\",\n",
    "        \"max_scale\":      \"maximum scale of boxes for data-set generation\",\n",
    "        \"min_pos\":        \"minimum x-position of boxes\",\n",
    "        \"max_pos\":        \"maximum x-position of boxes\",\n",
    "        \"max_h\":          \"maximum start-height of boxes\",\n",
    "        \"max_cnt\":        \"maximum count of boxes\",\n",
    "        \"circ_vel\":       \"velocity used for colliding drops\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-process Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key                  value                     description\n",
      "\n",
      "l_fac                4.0                       multiplicative factor of low-res sdf-patches\n",
      "par_cnt              50                        particle count for each patch\n",
      "surf                 0.5                       surface tolerance, specifies where to extract the patches\n",
      "var                  1                         count of down-samplings of the same data-set with different seeds\n",
      "use_tanh             1                         apply tanh on sdf-patches, after multiplication with factor\n",
      "h_fac                12.0                      multiplicative factor of high-res sdf-patches\n",
      "stride               2                         stride used for the generation of the patches\n",
      "factor               9                         goal up-scale factor of particles\n",
      "id                   p02                       id of the config version\n",
      "min_n                0                         minimum amount of neighbors used for down-sampling\n",
      "patch_size           5                         size of generated surface patches (of the low-res data)\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.dirname(config_path) + '/' + config['preprocess'], 'r') as f:\n",
    "    pre_config = json.loads(f.read())\n",
    "print_config(pre_config, {\n",
    "        \"id\":             \"id of the config version\",\n",
    "        \"patch_size\":     \"size of generated surface patches (of the low-res data)\",\n",
    "        \"stride\":         \"stride used for the generation of the patches\",\n",
    "        \"surf\":           \"surface tolerance, specifies where to extract the patches\",\n",
    "        \"l_fac\":          \"multiplicative factor of low-res sdf-patches\",\n",
    "        \"h_fac\":          \"multiplicative factor of high-res sdf-patches\",\n",
    "        \"use_tanh\":       \"apply tanh on sdf-patches, after multiplication with factor\",\n",
    "        \"factor\":         \"goal up-scale factor of particles\",\n",
    "        \"min_n\":          \"minimum amount of neighbors used for down-sampling\",\n",
    "        \"var\":            \"count of down-samplings of the same data-set with different seeds\",\n",
    "        \"par_cnt\":        \"particle count for each patch\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key                  value                     description\n",
      "\n",
      "features             ['ps']                    list of strings which the features used by the neural network\n",
      "val_split            0.2                       split ratio for train and validation data\n",
      "t_start              5                         timestep start-point of data\n",
      "learning_rate        0.001                     learing rate used for training\n",
      "train_split          0.9                       split ratio for train and test data\n",
      "epochs               2                         training epochs\n",
      "batch_size           32                        batch size used for training and validation\n",
      "adv_fac              0.0                       adversarial loss factor, if bigger then one a GAN architecture is used\n",
      "explicit             True                      specify if use a lagrangian- or eulerian-based network\n",
      "mse_fac              1.0                       MSE loss factor\n",
      "t_end                15                        timestep end-point of data\n",
      "id                   t02                       id of the config version\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.dirname(config_path) + '/' + config['train'], 'r') as f:\n",
    "    train_config = json.loads(f.read())\n",
    "print_config(train_config, {\n",
    "        \"id\":                 \"id of the config version\",\n",
    "        \"train_split\":        \"split ratio for train and test data\",\n",
    "        \"val_split\":          \"split ratio for train and validation data\",\n",
    "        \"t_start\":            \"timestep start-point of data\",\n",
    "        \"t_end\":              \"timestep end-point of data\",\n",
    "        \"features\":           \"list of strings which the features used by the neural network\",\n",
    "        \"batch_size\":         \"batch size used for training and validation\",\n",
    "        \"learning_rate\":      \"learing rate used for training\",\n",
    "        \"epochs\":             \"training epochs\",\n",
    "        \"explicit\":           \"specify if use a lagrangian- or eulerian-based network\",\n",
    "        \"mse_fac\":            \"MSE loss factor\",\n",
    "        \"adv_fac\":            \"adversarial loss factor, if bigger then one a GAN architecture is used\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Data (High Resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3 show_data.py type ref verbose 0 data 2d_data/ var 0 dataset 19 t_end 15 config config/version_03.txt scr tmp/ref.png t_start 14 manta manta/\n"
     ]
    }
   ],
   "source": [
    "from subprocess import Popen, PIPE \n",
    "from IPython.display import Image\n",
    "\n",
    "def run_python(script,param):\n",
    "    command = [\"python3\",script]\n",
    "    for k,v in param.items():\n",
    "        command += [k, str(v)]\n",
    "    print(\" \".join(command))\n",
    "    \n",
    "    proc = Popen(command, stdin=None, stdout=PIPE, stderr=PIPE)\n",
    "\n",
    "    for line in proc.stderr:\n",
    "        print(line.decode('utf-8'))\n",
    "\n",
    "if not os.path.exists(\"tmp\"):\n",
    "    os.makedirs(\"tmp\")\n",
    "    \n",
    "param = {\n",
    "    \"data\" : data_path,\n",
    "    \"manta\" : \"manta/\",\n",
    "    \"config\" : config_path,\n",
    "    \"verbose\" : 0,\n",
    "    \"t_start\" : train_config[\"t_end\"]-1,\n",
    "    \"t_end\" : train_config[\"t_end\"],\n",
    "    \"dataset\" : dataset,\n",
    "    \"var\": 0,\n",
    "    \"type\": \"ref\",\n",
    "    \"scr\" : \"tmp/ref.png\"\n",
    "}\n",
    "\n",
    "run_python(\"show_data.py\",param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tmp/ref.png\" width=\"400\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Data (Low Resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3 show_data.py type real verbose 0 data 2d_data/ var 0 dataset 19 t_end 15 config config/version_03.txt scr tmp/real.png t_start 14 manta manta/\n"
     ]
    }
   ],
   "source": [
    "param = {\n",
    "    \"data\" : data_path,\n",
    "    \"manta\" : \"manta/\",\n",
    "    \"config\" : config_path,\n",
    "    \"verbose\" : 0,\n",
    "    \"t_start\" : train_config[\"t_end\"]-1,\n",
    "    \"t_end\" : train_config[\"t_end\"],\n",
    "    \"dataset\" : dataset,\n",
    "    \"var\": 0,\n",
    "    \"type\": \"real\",\n",
    "    \"scr\" : \"tmp/real.png\"\n",
    "}\n",
    "\n",
    "run_python(\"show_data.py\",param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tmp/real.png\" width=\"400\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Data (Low Resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3 show_data.py type src verbose 0 data 2d_data/ var 0 dataset 19 t_end 15 config config/version_03.txt scr tmp/src.png t_start 14 manta manta/\n"
     ]
    }
   ],
   "source": [
    "param = {\n",
    "    \"data\" : data_path,\n",
    "    \"manta\" : \"manta/\",\n",
    "    \"config\" : config_path,\n",
    "    \"verbose\" : 0,\n",
    "    \"t_start\" : train_config[\"t_end\"]-1,\n",
    "    \"t_end\" : train_config[\"t_end\"],\n",
    "    \"dataset\" : dataset,\n",
    "    \"var\": 0,\n",
    "    \"type\": \"src\",\n",
    "    \"scr\" : \"tmp/src.png\"\n",
    "}\n",
    "\n",
    "run_python(\"show_data.py\",param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tmp/src.png\" width=\"400\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracted Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3 show_patches.py l_scr tmp/src_patch_%02d.png verbose 0 data 2d_data/ dataset 19 h_scr tmp/ref_patch_%02d.png timestep 14 config config/version_03.txt var 0 manta manta/\n"
     ]
    }
   ],
   "source": [
    "param = {\n",
    "    \"data\" : data_path,\n",
    "    \"manta\" : \"manta/\",\n",
    "    \"config\" : config_path,\n",
    "    \"verbose\" : 0,\n",
    "    \"timestep\" : train_config[\"t_end\"]-1,\n",
    "    \"dataset\" : dataset,\n",
    "    \"var\": 0,\n",
    "    \"l_scr\" : \"tmp/src_patch_%02d.png\",\n",
    "    \"h_scr\" : \"tmp/ref_patch_%02d.png\"\n",
    "}\n",
    "\n",
    "run_python(\"show_patches.py\",param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tmp/src_patch_29.png\" width=\"400\"/> \n",
    "<img src=\"tmp/ref_patch_29.png\" width=\"400\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2d_data/models/sph_2D_v03.h5\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5523caeceecc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s%s_%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"models/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prefix'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Subpixel'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSubpixel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SpatialTransformer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSpatialTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SplitDense'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSplitDense\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'adv_fac'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDiscriminator:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    312\u001b[0m                         \u001b[0;34m'Maybe you meant to use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                         '`Sequential.from_config(config)`?')\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 return cls.from_config(config['config'],\n\u001b[1;32m    139\u001b[0m                                        custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 140\u001b[0;31m                                                            list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m   2498\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munprocessed_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2499\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mnode_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munprocessed_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2500\u001b[0;31m                         \u001b[0mprocess_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2502\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mprocess_node\u001b[0;34m(layer, node_data)\u001b[0m\n\u001b[1;32m   2455\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2457\u001b[0;31m                     \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2458\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2459\u001b[0m                     \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mask'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpatialTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpar_cnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpar_cnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSplitDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "from subpixel import *\n",
    "from spatial_transformer import *\n",
    "from split_dense import *\n",
    "from hungarian_loss import HungarianLoss\n",
    "\n",
    "model_path = '%s%s_%s' % (data_path + \"models/\", data_config['prefix'], config['id'])\n",
    "print(model_path + \".h5\")\n",
    "load_model(model_path + \".h5\", custom_objects={'Subpixel': Subpixel, 'SpatialTransformer': SpatialTransformer, 'SplitDense': SplitDense, 'hungarian_loss':HungarianLoss(train_config['batch_size']).hungarian_loss}).summary()\n",
    "if train_config['adv_fac'] > 0:\n",
    "    print(\"\\nDiscriminator:\\n\")\n",
    "    load_model(model_path + \"_dis.h5\", custom_objects={'Subpixel': Subpixel}).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2d_data/models/sph_2D_v03_loss.png\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9f0d221a7dcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s_loss.png'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "fig_path = '%s_loss.png' % model_path\n",
    "print(fig_path)\n",
    "Image(filename=fig_path, width=400) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_python' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-45604bf15803>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mrun_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run.py\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'run_python' is not defined"
     ]
    }
   ],
   "source": [
    "# run network\n",
    "param = {\n",
    "    \"data\" : data_path,\n",
    "    \"config\" : config_path,\n",
    "    \"verbose\" : 0,\n",
    "    \"t_start\" : train_config[\"t_start\"],\n",
    "    \"t_end\" : train_config[\"t_end\"],\n",
    "    \"dataset\" : dataset,\n",
    "    \"var\": 0,\n",
    "    \"checkpoint\": 15\n",
    "}\n",
    "\n",
    "run_python(\"run.py\",param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_python' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f07162e915c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m }\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mrun_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"show_data.py\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'run_python' is not defined"
     ]
    }
   ],
   "source": [
    "param = {\n",
    "    \"data\" : data_path,\n",
    "    \"manta\" : \"manta/\",\n",
    "    \"config\" : config_path,\n",
    "    \"verbose\" : 0,\n",
    "    \"t_start\" : train_config[\"t_start\"],\n",
    "    \"t_end\" : train_config[\"t_end\"],\n",
    "    \"dataset\" : dataset,\n",
    "    \"var\": 0,\n",
    "    \"type\": \"res\",\n",
    "    \"scr\" : \"tmp/res_%03d.png\"\n",
    "}\n",
    "\n",
    "run_python(\"show_data.py\",param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tmp/res_014.png\" width=\"400\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3 run.py config config/version_03.txt checkpoint 15 t_start 5 verbose 0 var 0 t_end 15 dataset 19 data 2d_data/ src 2d_data/real/sph_2D_dat00_d019_%03d\n",
      "2017-12-07 08:37:54.934745: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "\n",
      "2017-12-07 08:37:54.934777: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "\n",
      "2017-12-07 08:37:54.934792: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "\n",
      "2017-12-07 08:37:54.934797: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "\n",
      "Using TensorFlow backend.\n",
      "\n",
      "Exception ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x11a36edd8>>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 701, in __del__\n",
      "\n",
      "TypeError: 'NoneType' object is not callable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run network\n",
    "param = {\n",
    "    \"data\" : data_path,\n",
    "    \"src\" : data_path + \"real/%s_%s_d%03d\" % (data_config['prefix'], data_config['id'], dataset) + \"_%03d\",\n",
    "    \"config\" : config_path,\n",
    "    \"verbose\" : 0,\n",
    "    \"t_start\" : train_config[\"t_start\"],\n",
    "    \"t_end\" : train_config[\"t_end\"],\n",
    "    \"dataset\" : dataset,\n",
    "    \"var\": 0,\n",
    "    \"checkpoint\":15\n",
    "}\n",
    "\n",
    "run_python(\"run.py\",param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3 show_data.py config config/version_03.txt manta manta/ t_start 5 scr tmp/real_res_%03d.png type res_real verbose 0 t_end 15 dataset 19 data 2d_data/ var 0\n"
     ]
    }
   ],
   "source": [
    "param = {\n",
    "    \"data\" : data_path,\n",
    "    \"manta\" : \"manta/\",\n",
    "    \"config\" : config_path,\n",
    "    \"verbose\" : 0,\n",
    "    \"t_start\" : train_config[\"t_start\"],\n",
    "    \"t_end\" : train_config[\"t_end\"],\n",
    "    \"dataset\" : dataset,\n",
    "    \"var\": 0,\n",
    "    \"type\": \"res_real\",\n",
    "    \"scr\" : \"tmp/real_res_%03d.png\"\n",
    "}\n",
    "\n",
    "run_python(\"show_data.py\",param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tmp/real_res_014.png\" width=\"400\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
